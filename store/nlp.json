[
  {
      "title": "Contoh Penerapan NLP",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Ada banyak contoh penggunaan NLP dalam kehidupan sehari-hari, bahkan bisa jadi kamu telah menggunakannya hampir setiap hari. Di bawah ini contoh penerapan NLP:</p><h3><strong>Pendeteksi spam</strong></h3><p>Pendeteksi spam seperti yang ada pada Google Mail menggunakan teknologi NLP untuk mendeteksi spam melalui klasifikasi kata.</p><h3><strong>Mesin penerjemah</strong></h3><p>Siapa yang belum pernah menggunakan Google Translate? Mesin penerjemah seperti Google Translate, Microsoft Translator, Linguee, dsb akan memudahkan kamu memahami tulisan asing dengan lebih cepat.</p><h3><strong>Virtual assistant dan chatbot</strong></h3><p>Asisten virtual seperti Siri dan Alexa menggunakan teknologi speech recognition untuk mengenali setiap suara dan merespons penggunanya dengan tindakan yang sesuai.</p><p>Ada juga <em>chatbot</em> yang dapat mengenali petunjuk kontekstual tentang permintaan manusia. Kemudian menggunakannya untuk memberikan respons atau opsi yang lebih baik dari waktu ke waktu.</p><h3><strong>Analisis sentimen media sosial</strong></h3><p>Analisis sentimen yang dilakukan oleh NLP dapat membantu kamu memahami lebih dalam bahasa yang digunakan pelanggan dalam posting media sosial, tanggapan, ulasan, serta lainnya. Dengan begitu, kamu dapat lebih mudah mengarahkan proses <em>design</em>, kampanye dan lainnya.</p><h3><strong>Text summarization</strong></h3><p>Mesin peringkas teks menggunakan teknologi NLP untuk menganalisis teks digital dalam jumlah besar dan membuat ringkasan dari teks tersebut dengan lebih cepat.</p>"
  },
  {
      "title": "Jenis-jenis NLP",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Setelah pemrosesan awal data, saatnya melakukan pengembangan algoritma. Ada dua jenis algoritma umum yang biasa digunakan dalam NLP:</p><h3><strong>Rules-based system</strong></h3><p>Jenis algoritma pertama yang digunakan dalam NLP adalah menggunakan aturan linguistik yang dirancang dengan teliti. <em>Rules-based system</em> telah digunakan pada awal pengembangan NLP dan masih digunakan hingga saat ini.</p><h3><strong>Machine learning-based system</strong></h3><p>Jenis algoritma kedua menggunakan metode statistik. Di mana komputer akan belajar melakukan tugas berdasarkan data pelatihan yang diberikan, dan menyesuaikan algoritma mereka saat lebih banyak data yang diproses.<br><em>Machine learning-based system</em> menggunakan kombinasi <em>machine learning, deep learning,</em> serta <em>neural networks.</em> Algoritma NLP jenis ini akan mengasah aturan mereka sendiri melalui pemrosesan dan pembelajaran berulang.</p>"
  },
  {
      "title": "Cara Kerja NLP",
      "category": [
          "Introduction",
          "Natural Language Processing (NLP)"
      ],
      "text": "<p><em>Natural Language Processing</em> (NLP) adalah bagian dari implementasi kecerdasan buatan yang bekerja dengan gabungan beberapa ilmu lain. Setidaknya, ada dua fase utama dalam proses kerja NLP, yaitu pemrosesan awal data dan pengembangan algoritma.</p><p>Pemrosesan awal data dalam NLP meliputi persiapan serta pembersihan data teks. Hal ini dilakukan untuk menempatkan data dalam bentuk yang dapat diterapkan pada algoritma. Sehingga, komputer dapat menganalisis data dengan baik.</p><p>Pemrosesan awal data dapat dilakukan dengan beberapa cara:</p><ol><li><p><strong><em>Part-of-speech tagging</em></strong>: Kata-kata ditandai berdasarkan kelas katanya, seperti kata benda, kata kerja, dan kata sifat.</p></li><li><p><strong><em>Stop word removal</em></strong>: Kata-kata umum dihapus dari teks sehingga kata-kata unik dengan informasi yang paling banyak tentang teks tetap ada.</p></li><li><p><strong><em>Lemmatization and stemmin</em></strong>g: Kata-kata direduksi ke bentuk dasarnya dengan menghapus awalan dan akhiran kata.</p></li><li><p><strong><em>Tokenization</em></strong>: Kata-kata dipecah menjadi unit yang lebih kecil untuk diproses.</p></li></ol>"
  },
  {
      "title": "Apa Itu NLP?",
      "category": [
          "Introduction",
          "Natural Language Processing (NLP)"
      ],
      "text": "<p><em>Natural Language Processing</em> atau disingkat NLP adalah bagian dari <em>Artificial Intelligence</em> (AI) yang berkaitan dengan memberikan kemampuan pada komputer untuk memahami bahasa alami manusia. Seperti tulisan maupun suara yang sering digunakan oleh manusia dalam percakapan sehari-hari.</p><p>NLP diciptakan dengan menggabungkan linguistik komputasi bersama model statistik, <em>machine learning</em>, dan <em>deep learning</em>. Semua unsur tersebut kemudian digunakan untuk membantu komputer memproses <em>(processing)</em> data teks maupun audio manusia. Membuatnya memahami data tersebut hingga mendapatkan makna penuh, lengkap dengan maksud dan sentimennya.</p><p>Kamu dapat menemukan kehadiran NLP pada perangkat komputer yang mampu dioperasikan dengan suara, asisten digital, perangkat lunak <em>text-to-speech, chatbot</em> layanan pelanggan, dan lain sebagainya. Selain itu, NLP juga berperan sebagai solusi modern perusahaan yang dapat membantu menghemat operasional, meningkatkan produktivitas, serta menyederhanakan proses bisnis yang sangat penting.</p>"
  },
  {
      "title": "Contoh Natural Language Processing (NLP)",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Terkait pengertian dan arti NLP di atas, kemudian seperti apa contoh-contoh <em>Natural Language Processing</em> (NLP) yang ada dalam kehidupan sehari-hari?</p><p>Memang, dalam beberapa tahun terakhir, karena ketersediaan <em>big data</em> (baca pengertian <em>big data</em> di sini), komputasi yang kuat, dan algoritma yang disempurnakan, <em>Natural Language Processing</em> (NLP) telah dengan cepat memajukan dan mengubah organisasi, bisnis dan perusahaan.</p><p><strong>Benar! NLP sekarang banyak digunakan di berbagai bidang dan industri seperti:</strong></p><ul><li><p><strong>Mesin telusur (<em>search engine</em>)</strong>; menggunakan pemrosesan bahasa alami untuk menghasilkan hasil penelusuran yang relevan berdasarkan perilaku penelusuran atau maksud pengguna yang serupa. Dengan menggunakan NLP, rata-rata orang menemukan apa yang mereka cari.</p></li><li><p><strong>Filter e-mail dan komentar</strong>; Filter spam telah ada selama beberapa waktu sekarang, tetapi klasifikasi e-mail milik Google yaitu Gmail adalah salah satu aplikasi NLP yang lebih baru. Mereka dapat mengenali mana dari 3 (tiga) kategori (utama, sosial, atau promosi) e-mail tersebut. Ini pastinya membantu pengguna menentukan e-mail mana yang penting dan memerlukan tanggapan cepat, dan yang mana yang mungkin ingin mereka hapus. Begitu juga seperti komentar yang ada di website.</p></li><li><p><strong>Industri terjemahan</strong>; Banyak perusahaan lokal yang menggunakan terjemahan mesin untuk membantu pegawai dan karyawan penerjemah mereka bekerja lebih efisien. Ya! Ketika teks sebagian besar sudah diterjemahkan oleh mesin, itu menghemat waktu mereka yang berharga dan jumlah kata yang dapat mereka terjemahkan setiap harinya akan meningkat pada datasetnya.</p></li><li><p><strong>Perawatan kesehatan</strong>; Ini dapat digunakan untuk menyederhanakan informasi pasien atau untuk aplikasi yang mengubah bahasa isyarat menjadi teks. Yang terakhir memungkinkan orang tuli untuk berkomunikasi dengan orang-orang yang tidak tahu bagaimana menggunakan bahasa isyarat.</p></li><li><p><strong>Teknik pesawat</strong>; Dalam contoh ini, <em>Natural Language Processing</em> (NLP) membantu mekanik menemukan informasi yang berguna dari manual pesawat yang memiliki ratusan halaman, dan membantu menemukan makna dalam deskripsi masalah yang dilaporkan oleh pilot atau orang lain yang bekerja di bisnis, organisasi atau perusahaan terkait.</p></li><li><p><strong>Asisten virtual</strong>; Asisten virtual seperti Windows Cortana, Google Assistant, dan Amazon Alexa dapat memahami dan menjawab pertanyaan Anda dengan metode pengenalan suara.</p></li></ul>"
  },
  {
      "title": "Natural Language Generation (NLG)",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Dalam komponen sistem ini, kita harus menghasilkan frasa dan kalimat yang bermakna, yakni berupa bahasa alami dari representasi internal.</p><p><strong>Proses dalam <em>Natural Language Generation</em> (NLU) ini seringkali melibatkan beberapa hal termasuk:</strong></p><ul><li><p><strong>Perencanaan teks (<em>text planning</em>)</strong>; Ini termasuk mengambil konten yang relevan dari basis pengetahuan.</p></li><li><p><strong>Perencanaan kalimat (<em>sentence planning</em>)</strong>; Ini adalah pemilihan kata-kata yang diperlukan, membentuk frase yang bermakna dan juga mengatur nada kalimat.</p></li><li><p><strong>Realisasi teks (<em>text realization</em>)</strong>; Merupakan <em>mapping</em> (pemetaan) rencana kalimat ke dalam struktur kalimat.</p></li></ul><p>NLG seringkali diterapkan dalam sistem chatbot, <em>text generation</em>, <em>voice assistants</em> dan <em>image captioning</em>.</p>"
  },
  {
      "title": "Natural Language Understanding (NLU)",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Jenis komponen sistem utama <em>Natural Language Processing</em> (NLP) yang pertama adalah <em>Natural Language Understanding</em> (NLU).</p><p>Dalam hal ini (NLU), kita harus memahami tugas dasar mereka pada intinya, yaitu memetakan input yang diberikan dalam bahasa alami ke representasi yang berguna.</p><p><strong>NLU memiliki bentuk dan struktur yang sangat kaya, dan seringkali sangat ambigu seperti beberapa hal sebagai berikut:</strong></p><ul><li><p><strong>Ambiguitas leksikal (<em>lexical ambiguity</em>)</strong>; Ini adalah tingkat yang sangat primitif seperti tingkat kata, sebagai contoh misalnya, dalam hal memperlakukan kata “board” sebagai kata benda atau kata kerja.</p></li><li><p><strong>Ambiguitas tingkat sintaks (<em>syntax level ambiguity</em>)</strong>; Ini adalah keadaan di mana sebuah kalimat dapat diuraikan dengan cara yang berbeda, sebagai contoh misalnya “He lifted the beetle with the red cap” yang bermakna apakah dia menggunakan topi untuk mengangkat kumbang atau dia mengangkat kumbang yang memiliki topi merah?</p></li><li><p><strong>Ambiguitas referensial (<em>referential ambiguity</em>)</strong>; Ini mengacu pada sesuatu menggunakan kata ganti, sebagai contoh misalnya, Rifqi berbicara ke Nanda dan berkata, “I am strong”, di mana komputer akan bingung dalam mengartikan siapa yang kuat.</p></li></ul><p>Seperti yang kalian lihat di atas, NLU dapat berupa satu atau banyak <em>input</em> (masukan) yang berarti (memiliki arti) hal yang sama, di mana mereka seringkali diaplikasikan ke analisis sentimen, penyaringan kata yang tidak senonoh, pengenalan suara, chatbot dan <em>text summarization</em>.</p>"
  },
  {
      "title": "Tujuan Menerapkan NLP dan Cara Kerjanya",
      "category": [
          "Advanced NLP",
          "Introduction",
          "Natural Language Processing (NLP)"
      ],
      "text": "<p>Setelah kita mengetahui apa itu arti <em>Natural Language Processing</em> (NLP), selanjutnya kita juga harus mengetahui untuk apa tujuan aplikasi dan penerapannya secara khusus.</p><p>Oke, secara khusus NLP menggerakkan program komputer yang menerjemahkan teks dari satu bahasa ke bahasa lain, menanggapi perintah lisan, dan meringkas teks dalam jumlah besar dengan cepat, bahkan secara <em>real-time</em>.</p><p>Ada kemungkinan besar Anda telah berinteraksi dengan NLP dalam bentuk sistem GPS yang dioperasikan dengan suara, asisten digital, perangkat lunak dikte <em>voice-to-text</em>, <em>chatbot</em> layanan pelanggan, dan kenyamanan konsumen lainnya.</p><p>Benar! NLP ini pastinya memainkan peran yang berkembang dalam solusi perusahaan yang membantu merampingkan operasi bisnis, meningkatkan produktivitas pegawai dan karyawan, serta menyederhanakan proses bisnis yang sangat penting.</p><p>Sederhananya, <em>Natural Language Processing</em> (NLP) memungkinkan komputer untuk memahami bahasa alami seperti yang dilakukan manusia.</p><p>Baik bahasa tersebut diucapkan atau ditulis, mereka menggunakan kecerdasan buatan (yang kita kenal dengan <em>Artificial Intelligence</em> atau AI) untuk mengambil input nyata, memprosesnya, dan memahaminya dengan cara yang dapat dipahami komputer.</p><p>Ini layaknya sensor yang berbeda seperti telinga manusia untuk mendengar, mata untuk melihat, begitu juga komputer yang memiliki program untuk membaca dan mikrofon untuk mengumpulkan <em>input</em> dari audio serta memungkinkan penerapan lanjutan dalam <em>hyperautomation</em> (otomasi hyper).</p><p><strong>Terkait cara kerjanya, perlu untuk diketahui bahwa ada 2 (dua) fase utama dalam <em>Natural Language Processing</em> (NLP) ini, yakni:</strong></p><ul><li><p><strong>Pemrosesan data</strong>; Ini melibatkan persiapan dan “pembersihan” data teks agar mesin dapat menganalisisnya.</p></li><li><p><strong>Pengembangan algoritma</strong>; <em>Preprocessing</em> dalam <em>Natural Language Processing</em> (NLP) menempatkan data dalam bentuk yang bisa diterapkan dan menyoroti fitur dalam teks yang dapat dikerjakan oleh suatu algoritme (baca pengertian dan tujuan algoritme di sini).</p></li></ul>"
  },
  {
      "title": "Apa itu Pemrosesan Bahasa Alami/Alamiah atau NLP?",
      "category": [
          "Advanced NLP",
          "Introduction",
          "Natural Language Processing (NLP)"
      ],
      "text": "<p>Jadi, apa itu sebenarnya yang dimaksud dengan <strong>pemrosesan bahasa alami/alamiah</strong> ini?</p><p>Seperti yang sudah Kami sebutkan di atas, istilah ini lebih dikenal dengan sebutan <em>Natural Language Processing</em> dan disingkat NLP.</p><p>Artinya secara luas, ini didefinisikan sebagai manipulasi otomatis bahasa alami, seperti ucapan dan teks, oleh <em>software</em> (perangkat lunak) yang membantu komputer untuk mengamati, menganalisis, memahami, dan memperoleh makna berharga dari (layaknya) bahasa lisan atau alami manusia.</p><p>Berbagai jenis dan macam <em>task</em> (tugas) seperti terjemahan, ringkasan otomatis, dan ekstraksi hubungan, pengenalan ucapan, pengenalan entitas bernama, segmentasi topik, dan analisis sentimen dapat dilakukan oleh pengembang menggunakan pemrosesan bahasa alami.</p><p>Penerapan dan aplikasi NLP memang terbilang sulit atau menantang karena komputer membutuhkan manusia untuk berinteraksi dengannya menggunakan bahasa pemrograman seperti Java, Python, dan lain sebagainya dengan syarat terstruktur dan tidak ambigu.</p><p>Akan tetapi bahasa lisan manusia seringkali bersifat ambigu (memiliki arti serta pengertian yang lebih dari satu) dan berubah dengan perubahan regional atau sosial, sehingga karena itulah NLP (<em>Natural Language Processing</em>) menjadi sulit untuk melatih komputer untuk memahami bahasa alami.</p>"
  },
  {
      "title": "Pengertian Natural Language Processing (NLP)",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Berarti <strong>pemrosesan bahasa alami/alamiah</strong> (dalam bahasa Indonesia), <strong><em>Natural Language Processing </em>(NLP)</strong> adalah mengacu pada cabang ilmu komputer, khususnya cabang <em>Artificial Intelligence</em> (AI) terkait praktik memberi instruksi pada komputer untuk memproses bahasa manusia pada umumnya.</p><p>Seperti yang juga dijelaskan menurut sumber simpulan Kami yang bersumber dari <strong>Situs IBM</strong>, NLP menggabungkan linguistik komputasi, pemodelan bahasa manusia yang berbasis dengan <em>rule</em> (aturan) yang ada serta dengan model statistik, pembelajaran mesin, dan pembelajaran mendalam.</p><p>Secara bersamaan, maka semua teknologi ini memungkinkan komputer untuk memproses bahasa manusia dalam bentuk teks atau data suara dan untuk “mengerti, memahami, dan mengetahui” makna sepenuhnya, lengkap dengan maksud dan sentimen dari pembicara atau penulisnya.</p>"
  },
  {
      "title": "Confusion Matrix",
      "category": [
          "Introduction"
      ],
      "text": "<p>Confusion Matrix merupakan salah satu metrik pengukuran performa untuk klasifikasi machine learning yang memiliki hasil keluaran dua kelas atau lebih. Penghitungan pada Confusion Matrix mempertimbangkan nilai antara hasil prediksi dengan nilai sesungguhnya. Terdapat 4 variabel yang digunakan Confusion 13 Matrix dalam melakukan perhitungan, yaitu True Positive (TP), True Negative (TN), False Positive (FP), dan False Negative (FN).</p>"
  },
  {
      "title": "XLM-R",
      "category": [
          "Introduction"
      ],
      "text":"<p>XLM-R atau Cross Lingual Model – RoBERTa adalah pengembangan dari XLM dan mBERT yang merupakan penelitian multilingual Natural Language Processing (NLP) model sebelumnya. XLM-R menggunakan transformer-based multilingual mask language model (MLM) yang sudah dilakukan pre-trained pada teks dengan 100 bahasa dan mampu menunjukkan performa yang sangat baik pada cross-lingual classification, sequence labeling, dan question answering (Conneau &amp; Khandelwal, 2020). XLM-R merupakan salah satu algoritma machine learning di dalam library Transformer, sebuah library yang menyediakan ribuan pretained model untuk melakukan pekerjaan seperti klasifikasi teks, information extraction, question answering, dan lain-lain. Pengembangan XLM-R memiliki tujuan untuk 10 meningkatkan kemampuan mesin atau komputer dalam melakukan multilingual Natural Language Processing (NLP) terutama pada low-resource languages.</p>"
  },
  {
      "title": "Multilingual Text Classification",
      "category": [
          "Pre-processing",
          "Introduction"
      ],
      "text": "<p>Text classification atau klasifikasi teks merupakan proses yang dilakukan oleh mesin atau komputer untuk mengkategorikan suatu teks ke dalam kelompok yang terorganisir. Text classification merupakan bagian dari NLP yang dapat menganalisa teks secara otomatis dan memberikan label atau kategori yang telah ditentukan sebelumnya berdasarkan konten dari teks tersebut. Text classification memungkinkan user untuk menjelajah dengan lebih mudah pada teks yang mereka 9 minati dan paradigma ini sangat efektif dalam penyaringan informasi seperti dalam pengembangan layanan secara online (Gonalves &amp; Quaresma, 2010). Walaupun kebanyakan dari penelitian mengenai text classification mengarah pada monolingual text classification, saat ini mulai banyak penelitian yang mengarah pada multilingual text classification. Multilingual text classification sendiri merupakan penelitian baru yang memungkinkan untuk melakukan text classification pada dokumen teks dengan berbagai bahasa. Dari kebanyakan penelitian mengenai multilingual text classification, model yang digunakan untuk melakukan klasifikasi mengandalkan training pada monolingual documents atau dokumen dengan satu bahasa, kemudian menggunakan translation mechanism atau mekanisme penerjemahan untuk melakukan klasifikasi dokumen yang tertulis dengan bahasa lain (Bel, 2003; Rigutini, 2005; Lee, 2009).</p>"
  },
  {
      "title": "Natural Language Processing",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Natural Language Processing (NLP) adalah rangkaian dari teknik komputasi untuk menganalisa dan merepresentasikan teks yang terjadi secara alami pada satu atau lebih tingkat analisis linguistik dengan tujuan mencapai pemrosesan bahasa yang menyerupai manusia untuk berbagai tugas atau aplikasi (Liddy &amp; D., 2001). NLP merupakan pengembangan dari Artificial Intelligence atau kecerdasan buatan yang diharapkan dapat mempelajari bahasa yang digunakan oleh manusia. Saat ini, NLP sudah banyak digunakan manusia seperti Google Translate, Google Assistant, Siri, Alexa, dan sebagainya. NLP memiliki banyak potensi untuk dikembangkan lebih lanjut, salah satunya adalah multilingual text classification yang akan dibahas lebih dalam pada penelitian ini.</p>"
  },
  {
      "attachment": {
          "title": "Arsitektur Sistem Aplikasi Membaca.pdf",
          "link": "https://firebasestorage.googleapis.com/v0/b/app-reading.appspot.com/o/Arsitektur%20Sistem%20Aplikasi%20Membaca.pdf?alt=media&token=7a1a8abd-6a9e-4da1-99c2-9277efc888ac"
      },
      "title": "Penerapan Natural Language Processing",
      "cover": "https://firebasestorage.googleapis.com/v0/b/app-reading.appspot.com/o/wutheringwaves_galley1701508264197.jpg?alt=media&token=84e2d8e0-fd49-4382-8b57-e6249c4b406c",
      "writer": {
          "_id": "64985e5cffc73ea565020a73",
          "username": "hunafa",
          "password": "zaky",
          "photo": "https://via.placeholder.com/300/3F51B5/263238.png?text=H",
          "work_list": [
              "65d69c42979b41d7867d3539",
              "65d69bcc979b41d7867d3528",
              "65d69b49979b41d7867d3517",
              "65d69b17979b41d7867d3506",
              "65d5fc2a979b41d7867d34f5",
              "65d5fc03979b41d7867d34e4",
              "65d5fb68979b41d7867d34d3",
              "65d5fb22979b41d7867d34c2",
              "65d5faf5979b41d7867d34b1",
              "65d5facf979b41d7867d34a0",
              "65d5d868979b41d7867d348f",
              "65d5d24f979b41d7867d347e",
              "65d5d1ff979b41d7867d3469",
              "65d5d1c8979b41d7867d3458",
              "6584356b0b4868262932671a",
              "65841d4e0b486826293266e9",
              "65841d210b486826293266de",
              "65841cdb0b486826293266d3",
              "65841cb20b486826293266c8",
              "65841c670b486826293266bd",
              "65841c240b486826293266b2",
              "65841bdb0b486826293266a7",
              "65841bb80b4868262932669c",
              "65841a760b48682629326691",
              "658419960b48682629326684",
              "658415d70b48682629326679",
              "658415af0b4868262932666e",
              "6584159b0b48682629326663",
              "658415850b48682629326658",
              "658415470b4868262932664d",
              "6584152f0b48682629326642",
              "6584151e0b48682629326637",
              "658414e00b4868262932662c",
              "658414930b48682629326621",
              "6584143b0b48682629326616",
              "6584134d0b4868262932660b",
              "658412540b48682629326600",
              "658412370b486826293265f5",
              "6584121a0b486826293265ea",
              "658410e60b486826293265df",
              "658410cb0b486826293265d4",
              "6584109e0b486826293265c9",
              "6584108c0b486826293265be",
              "6584104f0b486826293265b3",
              "65840fe50b486826293265a8"
          ],
          "read_list": [
              "6584356b0b4868262932671a",
              "65840fe50b486826293265a8"
          ],
          "like_list": [],
          "createdAt": "2023-06-25T15:33:48.921Z",
          "updatedAt": "2024-02-22T00:58:42.888Z",
          "__v": 0,
          "pen_name": "hunafa",
          "rate_list": [
              {
                  "work_id": "6584356b0b4868262932671a",
                  "rating": 5
              }
          ]
      },
      "readers": [
          {
              "_id": "658435f60b48682629326742",
              "username": "user2",
              "pen_name": "user2",
              "password": "user2",
              "photo": "https://via.placeholder.com/300/3F51B5/263238.png?text=U",
              "work_list": [],
              "read_list": [
                  "6584356b0b4868262932671a"
              ],
              "like_list": [],
              "rate_list": [
                  {
                      "work_id": "6584356b0b4868262932671a",
                      "rating": 4
                  }
              ],
              "createdAt": "2023-12-21T12:56:22.274Z",
              "updatedAt": "2023-12-21T12:56:47.228Z",
              "__v": 0
          },
          {
              "_id": "64985e5cffc73ea565020a73",
              "username": "hunafa",
              "password": "zaky",
              "photo": "https://via.placeholder.com/300/3F51B5/263238.png?text=H",
              "work_list": [
                  "65d69c42979b41d7867d3539",
                  "65d69bcc979b41d7867d3528",
                  "65d69b49979b41d7867d3517",
                  "65d69b17979b41d7867d3506",
                  "65d5fc2a979b41d7867d34f5",
                  "65d5fc03979b41d7867d34e4",
                  "65d5fb68979b41d7867d34d3",
                  "65d5fb22979b41d7867d34c2",
                  "65d5faf5979b41d7867d34b1",
                  "65d5facf979b41d7867d34a0",
                  "65d5d868979b41d7867d348f",
                  "65d5d24f979b41d7867d347e",
                  "65d5d1ff979b41d7867d3469",
                  "65d5d1c8979b41d7867d3458",
                  "6584356b0b4868262932671a",
                  "65841d4e0b486826293266e9",
                  "65841d210b486826293266de",
                  "65841cdb0b486826293266d3",
                  "65841cb20b486826293266c8",
                  "65841c670b486826293266bd",
                  "65841c240b486826293266b2",
                  "65841bdb0b486826293266a7",
                  "65841bb80b4868262932669c",
                  "65841a760b48682629326691",
                  "658419960b48682629326684",
                  "658415d70b48682629326679",
                  "658415af0b4868262932666e",
                  "6584159b0b48682629326663",
                  "658415850b48682629326658",
                  "658415470b4868262932664d",
                  "6584152f0b48682629326642",
                  "6584151e0b48682629326637",
                  "658414e00b4868262932662c",
                  "658414930b48682629326621",
                  "6584143b0b48682629326616",
                  "6584134d0b4868262932660b",
                  "658412540b48682629326600",
                  "658412370b486826293265f5",
                  "6584121a0b486826293265ea",
                  "658410e60b486826293265df",
                  "658410cb0b486826293265d4",
                  "6584109e0b486826293265c9",
                  "6584108c0b486826293265be",
                  "6584104f0b486826293265b3",
                  "65840fe50b486826293265a8"
              ],
              "read_list": [
                  "6584356b0b4868262932671a",
                  "65840fe50b486826293265a8"
              ],
              "like_list": [],
              "createdAt": "2023-06-25T15:33:48.921Z",
              "updatedAt": "2024-02-22T00:58:42.888Z",
              "__v": 0,
              "pen_name": "hunafa",
              "rate_list": [
                  {
                      "work_id": "6584356b0b4868262932671a",
                      "rating": 5
                  }
              ]
          }
      ],
      "like_by": [],
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>lorem ipsum</p>",
      "rate_by": [
          {
              "user_id": "658435f60b48682629326742",
              "rating": 4
          },
          {
              "user_id": "64985e5cffc73ea565020a73",
              "rating": 5
          }
      ],
      "createdAt": "2023-12-21T12:54:03.952Z",
      "updatedAt": "2023-12-21T12:56:47.231Z",
      "id": "6584356b0b4868262932671a"
  },
  {
      "title": "Text Summarization dengan NLP",
      "category": [
          "Advanced NLP"
      ],
      "text": "<p>Sangat sulit bagi manusia untuk meringkas dokumen teks besar secara manual. Ringkasan teks adalah masalah dalam NLP dalam membuat ringkasan singkat, akurat, dan lancar untuk dokumen sumber. Ini menjadi alat yang penting dan tepat waktu untuk membantu dan menafsirkan informasi teks di era informasi yang berkembang pesat saat ini. Dengan pemberitahuan push dan intisari artikel mendapatkan lebih banyak daya tarik, tugas menghasilkan ringkasan yang cerdas dan akurat untuk potongan teks yang panjang telah berkembang setiap hari.</p><p><strong>Bagaimana cara kerjanya?</strong></p><p>Peringkasan otomatis teks bekerja dengan terlebih dahulu menghitung frekuensi kata untuk seluruh dokumen teks. Kemudian, 100 kata yang paling umum disimpan dan diurutkan. Setiap kalimat kemudian dinilai berdasarkan berapa banyak kata frekuensi tinggi yang dikandungnya, dengan kata-kata frekuensi tinggi lebih berharga. Akhirnya, kalimat X teratas diambil dan diurutkan berdasarkan posisinya dalam teks asli.</p><p>Peringkasan Teks GIF</p><p>Dengan menjaga hal-hal sederhana dan untuk tujuan umum, algoritma peringkasan teks otomatis dapat berfungsi dalam berbagai situasi yang mungkin dihadapi oleh implementasi lain, seperti dokumen yang berisi bahasa asing atau asosiasi kata unik yang tidak ditemukan dalam korporat bahasa Inggris standar.</p><p>Ada dua pendekatan mendasar untuk peringkasan teks: <strong>ekstraktif </strong>dan <strong>abstraktif</strong>. Yang pertama mengekstrak kata dan frasa kata dari teks asli untuk membuat ringkasan. Yang terakhir belajar representasi bahasa internal untuk menghasilkan lebih banyak ringkasan seperti manusia, memparafrasekan maksud dari teks asli.</p><p>Metode dalam <strong>peringkasan ekstraktif</strong> bekerja dengan memilih subset. Ini dilakukan dengan mengekstrak frasa atau kalimat dari artikel yang sebenarnya untuk membentuk ringkasan. <strong>LexRank</strong> dan <strong>TextRank</strong> adalah ringkasan ekstraktif yang terkenal. Keduanya menggunakan variasi algoritma Google PageRank.</p><ul><li><p>LexRank adalah algoritma berbasis grafik tanpa pengawasan yang menggunakan IDF-modified Cosine sebagai ukuran kesamaan antara dua kalimat. Kesamaan ini digunakan sebagai bobot tepi grafik antara dua kalimat. LexRank juga menggabungkan langkah pasca-pemrosesan cerdas yang memastikan kalimat teratas yang dipilih untuk ringkasan tidak terlalu mirip satu sama lain.</p></li><li><p>TextRank is a similar algorithm to LexRank with a few enhancements, such as using lemmatization instead of stemming, incorporating Part-Of-Speech tagging and Named Entity Resolution, extracting key phrases from the article, and extracting summary sentences based on those phrases. Along with a summary of the article, TextRank also extracts meaningful key phrases from the article.</p></li></ul><p>Terjemahan Teks</p><p>Model untuk <strong>peringkasan abstraktif</strong> berada di bawah payung pembelajaran mendalam yang lebih besar. Ada terobosan tertentu dalam peringkasan teks menggunakan pembelajaran mendalam. Berikut adalah beberapa hasil publikasi yang paling menonjol oleh beberapa perusahaan terbesar di bidang NLP:</p><ul><li><p>Neural Attention Facebook adalah arsitektur jaringan saraf yang menggunakan model berbasis perhatian lokal yang mampu menghasilkan setiap kata dari ringkasan yang dikondisikan pada kalimat input.</p></li><li><p>Model Sequence-to-Sequence Google Brain mengikuti arsitektur encoder-decoder. Encoder bertanggung jawab untuk membaca dokumen sumber dan menyandikannya ke representasi internal. Dekoder adalah model bahasa yang bertanggung jawab untuk menghasilkan setiap kata dalam ringkasan output menggunakan representasi dokumen sumber yang dikodekan.</p></li><li><p>IBM Watson menggunakan model Sequence-to-Sequence yang serupa, tetapi dengan perhatian dan fitur jaringan saraf berulang dua arah.</p></li></ul>"
  },
  {
      "title": "Sistem QNA dengan NLP Tingkat Lanjut",
      "category": [
          "Advanced NLP"
      ],
      "text": "<p>Ide dari <strong>sistem Question Answering (QA)</strong> adalah untuk mengekstrak informasi, langsung dari dokumen, percakapan, pencarian online, dan di tempat lain, yang akan memenuhi kebutuhan informasi pengguna. Daripada membuat pengguna membaca seluruh dokumen, sistem QA lebih suka memberikan jawaban singkat dan ringkas. Saat ini, sistem QA dapat menggabungkan dengan sangat mudah dengan sistem NLP lain seperti chatbots, dan beberapa sistem QA bahkan melampaui pencarian dokumen teks dan dapat mengekstrak informasi dari kumpulan gambar.</p><p>Bahkan, sebagian besar masalah NLP dapat dianggap sebagai masalah menjawab pertanyaan. Paradigmanya sederhana: kami mengeluarkan kueri, dan mesin merespons. Dengan membaca dokumen, atau serangkaian instruksi, sistem cerdas harus dapat menjawab berbagai pertanyaan. Jadi tentu saja, kami ingin merancang model yang dapat digunakan untuk QA umum.</p><p>Pertanyaan dan Jawaban GIF</p><p>Arsitektur pembelajaran mendalam yang kuat, yang dikenal sebagai jaringan memori dinamis (DMN), telah dikembangkan dan dioptimalkan secara khusus untuk masalah QA. Dengan serangkaian pelatihan urutan input (pengetahuan) dan pertanyaan, itu dapat membentuk ingatan episodik, dan menggunakannya untuk menghasilkan jawaban yang relevan. Arsitektur memiliki komponen-komponen berikut:</p><ul><li><p><strong>Modul Memori Semantik</strong> (analog dengan basis pengetahuan) terdiri dari vektor GloVe yang telah dilatih sebelumnya yang digunakan untuk membuat urutan penyematan kata dari kalimat input. Vektor ini akan bertindak sebagai input ke model.</p></li><li><p><strong>Modul Input</strong> memproses vektor input yang terkait dengan pertanyaan menjadi satu set vektor yang disebut <em>fakta</em>. Modul ini diimplementasikan menggunakan <strong>Gated Recurrent Unit</strong>. GRU memungkinkan jaringan untuk mengetahui apakah kalimat yang saat ini sedang dipertimbangkan relevan atau tidak ada hubungannya dengan jawabannya.</p></li><li><p><strong>Modul Pertanyaan</strong> memproses pertanyaan kata demi kata dan menghasilkan vektor menggunakan GRU yang sama dengan modul input, dan bobot yang sama. Baik fakta maupun pertanyaan dikodekan sebagai penyematan.</p></li><li><p><strong>Modul Memori Episodik</strong> menerima vektor fakta dan pertanyaan yang diekstraksi dari input dan dikodekan sebagai embedding. Ini menggunakan proses yang terinspirasi oleh hippocampus otak, yang dapat mengambil keadaan temporal yang dipicu oleh beberapa respons, seperti pemandangan atau suara.</p></li><li><p>Finally, the <strong>Answer Module</strong> generates an appropriate response. By the final pass, the episodic memory should contain all the information required to answer the question. This module uses another GRU, trained with the cross-entropy error classification of the correct sequence, which can then be converted back to natural language.</p></li></ul><p>Dynamic Memory Network(DMN)</p><p>DMN tidak hanya bekerja sangat baik untuk tugas QA tetapi juga mengungguli arsitektur lain untuk analisis sentimen dan penandaan part-of-speech. Sejak awal, telah ada perbaikan besar pada Jaringan Memori Dinamis untuk lebih meningkatkan akurasi mereka pada tugas menjawab pertanyaan, termasuk:</p><ul><li><p>Jaringan Memori Dinamis untuk Jawaban Pertanyaan Visual dan Tekstual pada dasarnya adalah DMN yang diterapkan pada gambar. Modul memori dan inputnya ditingkatkan untuk dapat menjawab pertanyaan visual. Model ini meningkatkan keadaan seni pada banyak dataset Penjawab Pertanyaan Visual benchmark tanpa mendukung pengawasan fakta.</p></li><li><p>Jaringan Coattention Dinamis untuk Menjawab Pertanyaan membahas masalah pemulihan dari maksim lokal yang sesuai dengan jawaban yang salah. Ini pertama-tama menggabungkan representasi yang saling bergantung dari pertanyaan dan dokumen untuk fokus pada bagian yang relevan dari keduanya. Kemudian, decoder penunjuk dinamis mengulangi rentang jawaban potensial. Prosedur berulang ini memungkinkan model untuk pulih dari maksima lokal awal yang sesuai dengan jawaban yang salah.</p></li></ul>"
  },
  {
      "title": "Analisis Sentimen",
      "category": [
          "Advanced NLP"
      ],
      "text": "<p>Komunikasi manusia bukan hanya kata-kata dan makna eksplisitnya. Sebaliknya, itu bernuansa dan kompleks. Anda dapat mengetahui berdasarkan cara seorang teman mengajukan pertanyaan apakah mereka bosan, marah, atau penasaran. Anda dapat mengetahui berdasarkan pilihan kata dan tanda baca apakah pelanggan menjadi marah, bahkan dalam percakapan yang sepenuhnya berbasis teks.</p><p>GIF Analisis Sentimen</p><p>Anda dapat membaca ulasan Amazon untuk suatu produk dan memahami apakah pengulas menyukainya atau tidak menyukainya bahkan jika mereka tidak pernah mengatakannya secara langsung.</p><p>Agar komputer benar-benar memahami cara manusia berkomunikasi setiap hari, mereka perlu memahami lebih dari sekadar definisi objektif kata-kata; Mereka perlu memahami sentimen kita, apa yang sebenarnya kita maksudkan. Analisis sentimen adalah proses menafsirkan makna unit teks yang lebih besar (entitas, istilah deskriptif, fakta, argumen, cerita) dengan komposisi semantik elemen yang lebih kecil.</p><p>Pendekatan tradisional untuk analisis sentimen adalah memperlakukan kalimat sebagai sekantong kata dan berkonsultasi dengan daftar kata-kata \"positif\" dan \"negatif\" yang dikuratori untuk menentukan sentimen kalimat tertentu. Ini akan membutuhkan fitur yang dirancang dengan tangan untuk menangkap sentimen, yang sangat memakan waktu dan tidak dapat diskalakan.</p><p>Pendekatan pembelajaran mendalam modern untuk analisis sentimen dapat digunakan untuk morfologi, sintaksis, dan semantik logis, yang paling efektif adalah Jaringan Saraf Rekursif. Sesuai namanya, asumsi utama untuk pengembangan Recursive Neural Net adalah sedemikian rupa sehingga rekursi adalah cara alami untuk menggambarkan bahasa. Rekursi berguna dalam disambiguasi, berguna untuk beberapa tugas untuk merujuk pada frasa tertentu, dan bekerja sangat baik untuk tugas-tugas yang menggunakan struktur pohon gramatikal.</p><p>Kluster kata sentimen.</p><p>Jaringan Saraf Rekursif sangat cocok untuk pengaturan yang memiliki hierarki bersarang dan struktur rekursif intrinsik. Jika kita berpikir tentang sebuah kalimat, bukankah ini memiliki struktur seperti itu?</p><p>Ambil kalimat \"Kerumunan besar dengan kejam menyerang polisi yang tidak bersenjata.\" Pertama, kami memecah kalimat menjadi Frasa Kata Benda dan Frasa Kata Kerja masing-masing - \"Kerumunan besar\" dan \"dengan keras menyerang polisi yang tidak bersenjata.\" Tapi ada frasa kata benda dalam frasa kata kerja itu, bukan? \"serangan kekerasan\" dan \"polisi tak bersenjata.\" Tampaknya cukup rekursif.</p><p>Aturan sintaksis bahasa sangat rekursif. Jadi kami memanfaatkan struktur rekursif itu dengan model yang menghormatinya! Manfaat tambahan lain dari pemodelan kalimat dengan RNN adalah bahwa kita sekarang dapat memasukkan kalimat dengan panjang sewenang-wenang, yang merupakan penggaruk kepala besar untuk menggunakan Neural Nets di NLP, dengan trik yang sangat cerdas untuk membuat vektor input kalimat menjadi ukuran yang sama, meskipun panjang kalimat tidak sama.</p><p>RNN Standar adalah versi paling dasar dari Jaringan Saraf Rekursif. Ini memiliki arsitektur prediksi struktur max-margin yang dapat berhasil memulihkan struktur tersebut baik dalam gambar adegan yang kompleks maupun kalimat. Ini digunakan untuk menyediakan parser sintaksis kompetitif untuk kalimat bahasa alami dari Penn Treebank.</p><p>Untuk referensi Anda, Penn Treebank adalah dataset treebank skala besar ke-1 yang terdiri dari 2.499 cerita dari koleksi Wall Street Journal (WSJ) tiga tahun dari 98.732 cerita untuk anotasi sintaksis. Selain itu, ia mengungguli pendekatan alternatif untuk segmentasi adegan semantik, anotasi, dan klasifikasi.</p><p>Namun, RNN standar tidak menangkap kekayaan sintaksis penuh maupun semantik dari frasa linguistik. Syntactically Untied RNN, atau dikenal sebagai Compositional Vector Grammar (CVG), adalah peningkatan besar yang mengatasi masalah ini. Ini menggunakan jaringan saraf rekursif yang tidak terikat secara sintaksis yang mempelajari representasi vektor sintaksis-semantik dan komposisi. Model ini cepat dilatih dan diimplementasikan seefisien RNN standar. Ini mempelajari gagasan lembut tentang kata kunci dan meningkatkan kinerja pada jenis ambiguitas yang memerlukan informasi semantik.</p><p>Evolusi lain adalah Matrix-Vector RNN, yang mampu menangkap makna komposisi dari frasa yang lebih panjang. Model ini memberikan vektor dan matriks ke setiap node dalam pohon parse: vektor menangkap makna yang melekat pada konstituen, sedangkan matriks menangkap bagaimana ia mengubah makna kata atau frasa tetangga. RNN matriks-vektor ini dapat mempelajari arti operator dalam logika proposisional dan bahasa alami.</p><p>Akibatnya, model memperoleh keadaan kinerja seni pada tiga eksperimen yang berbeda:</p><ul><li><p>Memprediksi distribusi sentimen berbutir halus dari pasangan kata keterangan-kata sifat.</p></li><li><p>Mengklasifikasikan label sentimen ulasan film.</p></li><li><p>Mengklasifikasikan hubungan semantik seperti sebab-akibat atau topik-pesan antara kata benda menggunakan jalur sintaksis di antara mereka.</p></li></ul><p>Jaringan Tensor Neural Rekursif</p><p>Model RNN yang paling kuat untuk analisis sentimen yang dikembangkan sejauh ini adalah Recursive Neural Tensor Network, yang memiliki struktur pohon dengan jaring saraf di setiap node. Model ini dapat digunakan untuk segmentasi batas untuk menentukan kelompok kata mana yang positif dan mana yang negatif. Hal yang sama berlaku untuk kalimat secara keseluruhan. Ketika dilatih di Sentiment Treebank, model ini mengungguli semua metode sebelumnya pada beberapa metrik lebih dari 5%. Saat ini, ini adalah satu-satunya model yang dapat secara akurat menangkap efek negasi dan cakupannya di berbagai tingkat pohon untuk frasa positif dan negatif.</p>"
  },
  {
      "title": "Advanced Natural Language Processing - Dialogue and Conversations",
      "category": [
          "Natural Language Processing (NLP)",
          "Advanced NLP"
      ],
      "text": "<p>Banyak yang telah ditulis tentang AI percakapan, dan sebagian besar berfokus pada chatbots vertikal, platform messenger, tren bisnis, dan peluang startup (Amazon Alexa, Apple Siri, Facebook M, Asisten Google, Microsoft Cortana). Kemampuan AI untuk memahami bahasa alami masih terbatas. Akibatnya, membuat asisten percakapan domain terbuka yang sepenuhnya otomatis tetap menjadi tantangan terbuka. Meskipun demikian, pekerjaan yang ditunjukkan di bawah ini berfungsi sebagai titik awal yang bagus bagi orang-orang yang ingin mencari terobosan berikutnya dalam percakapan AI.</p><p>Asisten AI</p><p>Para peneliti dari Montreal, Georgia Tech, Microsoft, dan Facebook membangun jaringan saraf yang mampu menghasilkan respons percakapan yang sensitif terhadap konteks. Sistem pembuatan respons baru ini dapat dilatih secara end-to-end pada sejumlah besar percakapan Twitter yang tidak terstruktur. Arsitektur Jaringan Saraf Berulang digunakan untuk mengatasi masalah sparsitas yang muncul ketika mengintegrasikan informasi kontekstual ke dalam model statistik klasik, memungkinkan sistem untuk memperhitungkan ucapan dialog sebelumnya. Model ini menunjukkan peningkatan yang konsisten atas baseline Machine Translation dan Information Retrieval yang sensitif terhadap konteks dan non-konteks.</p><p>Mekanisme Encode Decoder</p><p>Dikembangkan di Hong Kong, Neural Responding Machine (NRM) adalah generator respons berbasis jaringan saraf untuk percakapan teks pendek. Dibutuhkan kerangka encoder-decoder umum. Pertama, ini memformalkan generasi respons sebagai proses decoding berdasarkan representasi laten dari teks input, sementara encoding dan decoding direalisasikan dengan Jaringan Saraf Berulang. NRM dilatih dengan sejumlah besar data percakapan satu putaran yang dikumpulkan dari layanan microblogging. Studi empiris menunjukkan bahwa NRM dapat menghasilkan tanggapan yang benar secara tata bahasa dan sesuai konten untuk lebih dari 75% teks input, mengungguli state-of-the-art dalam pengaturan yang sama.</p><p>Encoder-Decoder Machine Translation.</p><p>Last but not least, Model Percakapan Neural Google adalah pendekatan sederhana untuk pemodelan percakapan. Ini menggunakan kerangka kerja urutan-ke-urutan. Model berkomunikasi dengan memprediksi kalimat berikutnya yang diberikan kalimat sebelumnya dalam percakapan. Kekuatan modelnya sedemikian rupa sehingga dapat dilatih ujung ke ujung dan karenanya membutuhkan aturan kerajinan tangan yang jauh lebih sedikit.</p><p>Kode-Decoder GIF</p><p>Model ini dapat menghasilkan percakapan sederhana dengan dataset pelatihan percakapan yang besar. Ini dapat mengekstrak pengetahuan dari himpunan data khusus domain, dan dari kumpulan data domain subtitel film yang besar, berisik, dan umum. Pada himpunan data meja bantuan TI khusus domain, model dapat menemukan solusi untuk masalah teknis melalui percakapan. Pada himpunan data transkrip film domain terbuka yang bising, model dapat melakukan bentuk sederhana penalaran akal sehat.</p>"
  },
  {
      "title": "Terjemahan Mesin dengan NLP Tingkat Lanjut",
      "category": [
          "Advanced NLP"
      ],
      "text": "<p>Terjemahan Mesin adalah tes klasik pemahaman bahasa. Ini terdiri dari analisis bahasa dan generasi bahasa. Sistem terjemahan mesin besar memiliki penggunaan komersial yang besar, bahasa global adalah industri senilai $ 40 Miliar per tahun. Untuk memberi Anda beberapa contoh penting:</p><ul><li><p>Google Translate melewati 100 miliar kata per hari.</p></li><li><p>Facebook menggunakan terjemahan mesin untuk menerjemahkan teks dalam posting dan komentar secara otomatis, untuk memecahkan hambatan bahasa, dan memungkinkan orang di seluruh dunia untuk berkomunikasi satu sama lain.</p></li><li><p>eBay menggunakan teknologi Machine Translation untuk memungkinkan perdagangan lintas batas dan menghubungkan pembeli dan penjual di seluruh dunia.</p></li><li><p>Microsoft menghadirkan terjemahan bertenaga AI kepada pengguna akhir dan pengembang di Android, iOS, dan Amazon Fire, baik mereka memiliki akses ke Internet atau tidak.</p></li><li><p>Systran menjadi penyedia perangkat lunak ke-1 yang meluncurkan mesin Neural Machine Translation dalam lebih dari 30 bahasa pada tahun 2016.</p></li></ul><p>Dalam sistem Terjemahan Mesin tradisional, kita harus menggunakan korpus paralel - kumpulan teks, yang masing-masing diterjemahkan ke dalam satu atau lebih bahasa lain dari aslinya.</p><p>Misalnya, mengingat bahasa sumber \"f\" (misalnya Prancis) dan bahasa target \"e<strong>\"</strong> (misalnya bahasa Inggris), kita perlu membangun beberapa model statistik, termasuk formulasi probabilistik menggunakan aturan Bayesian, model terjemahan p (<strong>f</strong> | e) dilatih pada korpus paralel, dan model bahasa <strong>p (e)</strong> dilatih pada korpus bahasa Inggris saja.</p><p>Tak perlu dikatakan, <strong>pendekatan ini melewatkan ratusan detail penting</strong>, membutuhkan banyak rekayasa fitur manusia, terdiri dari banyak masalah pembelajaran mesin yang berbeda &amp; independen, dan secara keseluruhan adalah sistem yang sangat kompleks.</p><p><strong>Neural Machine Translation</strong> adalah pendekatan pemodelan seluruh proses ini melalui satu jaringan saraf tiruan besar, yang dikenal sebagai Recurrent Neural Network (RNN).</p><p><strong>RNN</strong> adalah jaringan saraf stateful, di mana ia memiliki koneksi antara lintasan, koneksi melalui waktu. Neuron diberi informasi tidak hanya dari lapisan sebelumnya tetapi juga dari diri mereka sendiri dari lintasan sebelumnya. Ini berarti bahwa urutan di mana kita memberi makan input dan melatih jaringan penting: memberinya makan \"Donald\" dan kemudian \"Trump\" dapat menghasilkan hasil yang berbeda dibandingkan dengan memberinya makan \"Trump\" dan kemudian \"Donald\".</p><p>Jaringan Neural Berulang untuk Terjemahan Mesin</p><p>GIF Terjemahan Mesin</p><p>Standard Neural Machine Translation adalah jaringan saraf end-to-end di mana kalimat sumber dikodekan oleh RNN yang disebut encoder, dan kata-kata target diprediksi menggunakan RNN lain yang dikenal sebagai decoder. RNN Encoder membaca kalimat sumber satu simbol pada satu waktu dan kemudian merangkum seluruh kalimat sumber dalam keadaan tersembunyi terakhirnya. RNN Decoder menggunakan propagasi balik untuk mempelajari ringkasan ini dan mengembalikan versi terjemahan. Hebatnya, Neural Machine Translation beralih dari kegiatan penelitian pinggiran pada tahun 2014 menjadi cara terkemuka yang diadopsi secara luas untuk melakukan Terjemahan Mesin pada tahun 2016. Jadi apa kemenangan besar menggunakan Neural Machine Translation?</p><ol><li><p><strong>Pelatihan menyeluruh</strong>: Semua parameter dalam Neural Machine Translation dioptimalkan secara bersamaan untuk meminimalkan fungsi kehilangan pada output jaringan.</p></li><li><p><strong>Representasi terdistribusi berbagi kekuatan</strong>: Neural Machine Translation memiliki eksploitasi yang lebih baik terhadap kesamaan kata dan frasa.</p></li><li><p><strong>Eksplorasi konteks yang</strong> lebih baik: Neural Machine Translation dapat menggunakan konteks yang jauh lebih besar — baik teks target sumber maupun parsial — untuk menerjemahkan dengan lebih akurat.</p></li><li><p><strong>Pembuatan</strong> teks yang lebih lancar: Pembuatan teks pembelajaran mendalam memiliki kualitas yang jauh lebih tinggi daripada cara korpus paralel.</p></li></ol><p>Satu masalah besar dengan RNN adalah <strong>masalah gradien</strong> <strong>yang menghilang</strong> (atau meledak) di mana, tergantung pada fungsi aktivasi yang digunakan, informasi dengan cepat hilang seiring waktu. Secara intuitif, ini tidak akan menjadi masalah karena ini hanya bobot dan bukan keadaan neuron, tetapi bobot melalui waktu sebenarnya adalah tempat informasi dari masa lalu disimpan; Jika bobotnya mencapai nilai 0 atau 1.000.000, keadaan sebelumnya tidak akan terlalu informatif. Akibatnya, RNN akan mengalami kesulitan dalam menghafal kata-kata sebelumnya sangat jauh dalam urutan dan hanya mampu membuat prediksi berdasarkan kata-kata terbaru.</p><p><strong>Jaringan memori jangka pendek panjang (LSTM)</strong> mencoba untuk memerangi masalah gradien menghilang / meledak dengan memperkenalkan gerbang dan sel memori yang didefinisikan secara eksplisit. Setiap neuron memiliki sel memori dan tiga gerbang: <strong>input, output, dan forget</strong>. Fungsi gerbang ini adalah untuk menjaga informasi dengan menghentikan atau membiarkan alirannya.</p><ul><li><p>Gerbang input menentukan berapa banyak informasi dari lapisan sebelumnya yang disimpan dalam sel.</p></li><li><p>Lapisan output mengambil pekerjaan di ujung yang lain dan menentukan berapa banyak lapisan berikutnya yang diketahui tentang keadaan sel ini.</p></li><li><p>Gerbang lupakan tampak seperti inklusi yang aneh pada awalnya, tetapi kadang-kadang baik untuk dilupakan: jika itu mempelajari buku dan bab baru dimulai, mungkin perlu bagi jaringan untuk melupakan beberapa karakter dari bab sebelumnya.</p></li></ul><p>LSTM GIF</p><p>LSTM dapat mempelajari urutan yang kompleks, seperti menulis seperti Shakespeare atau menggubah musik primitif. Perhatikan bahwa masing-masing gerbang ini memiliki bobot ke sel di neuron sebelumnya, sehingga mereka biasanya membutuhkan lebih banyak sumber daya untuk dijalankan. LSTM saat ini sangat hip dan telah banyak digunakan dalam terjemahan mesin. Selain itu, Ini adalah model default untuk sebagian besar tugas pelabelan urutan, yang memiliki banyak data.</p><p>Arsitektur LSTM</p><p><strong>Unit berulang terjaga keamanannya</strong> (GRU) adalah sedikit variasi pada LSTM dan juga merupakan ekstensi dari Neural Machine Translation. Mereka memiliki satu gerbang kurang dan kabel sedikit berbeda: alih-alih input, output, dan gerbang lupa, mereka memiliki gerbang pembaruan. Gerbang pembaruan ini menentukan berapa banyak informasi yang harus disimpan dari keadaan terakhir dan berapa banyak informasi yang diizinkan masuk dari lapisan sebelumnya.</p><p>Arsitektur GRU</p><p>Gerbang reset berfungsi seperti gerbang lupa LSTM, tetapi letaknya sedikit berbeda. Mereka selalu mengirimkan keadaan penuh mereka - mereka tidak memiliki gerbang keluaran. Dalam kebanyakan kasus, mereka berfungsi sangat mirip dengan LSTM, dengan perbedaan terbesar adalah bahwa GRU sedikit lebih cepat dan lebih mudah dijalankan (tetapi juga sedikit kurang ekspresif). Dalam praktiknya, ini cenderung membatalkan satu sama lain, karena Anda memerlukan jaringan yang lebih besar untuk mendapatkan kembali ekspresif, yang pada gilirannya membatalkan manfaat kinerja. Dalam beberapa kasus di mana ekspresi ekstra tidak diperlukan, GRU dapat mengungguli LSTM.</p><p>LSTM dan GRU</p><p>Selain 3 arsitektur utama ini, ada perbaikan lebih lanjut dalam sistem terjemahan mesin saraf selama beberapa tahun terakhir. Di bawah ini adalah perkembangan yang paling menonjol:</p><ul><li><p>Sequence to Sequence Learning with Neural Networks membuktikan efektivitas LSTM untuk Neural Machine Translation. Ini menyajikan pendekatan end-to-end umum untuk pembelajaran urutan yang membuat asumsi minimal pada struktur urutan. Metode ini menggunakan LSTM berlapis-lapis untuk memetakan urutan input ke vektor dimensi tetap, dan kemudian LSTM dalam lainnya untuk memecahkan kode urutan target dari vektor.</p></li><li><p>Neural Machine Translation by Jointly Learning to Align and Translate memperkenalkan mekanisme perhatian dalam NLP. Mengakui bahwa penggunaan vektor panjang tetap adalah hambatan dalam meningkatkan kinerja NMT, penulis mengusulkan untuk memperluas ini dengan memungkinkan model untuk secara otomatis (soft-) mencari bagian-bagian dari kalimat sumber yang relevan untuk memprediksi kata target, tanpa harus membentuk bagian-bagian ini seperti segmen keras secara eksplisit.</p></li><li><p>Convolutional over Recurrent Encoder for Neural Machine Translation menambah encoder RNN standar di NMT (Neural Machine Translation) dengan lapisan konvolusional tambahan untuk menangkap konteks yang lebih luas dalam output encoder.</p></li><li><p>Google membangun sistem NMT (Neural Machine Translation) sendiri, yang disebut Google's Neural Machine Translation, yang membahas banyak masalah dalam akurasi dan kemudahan penyebaran. Model ini terdiri dari jaringan LSTM yang dalam dengan 8 lapisan encoder dan 8 decoder menggunakan koneksi residual serta koneksi perhatian dari jaringan decoder ke encoder.</p></li><li><p>Alih-alih menggunakan Jaringan Saraf Berulang, Peneliti AI Facebook menggunakan jaringan saraf konvolusional untuk urutan untuk mengurutkan tugas belajar di NMT (Neural Machine Translation).</p></li></ul>"
  },
  {
      "title": "Penyematan Teks Dengan NLP Tingkat Lanjut",
      "category": [
          "Advanced NLP"
      ],
      "text": "<p>Dalam NLP tradisional, kita menganggap kata-kata sebagai simbol diskrit, yang kemudian dapat diwakili oleh vektor satu-panas. Dimensi vektor adalah jumlah kata dalam seluruh kosakata. Masalah dengan kata-kata sebagai simbol diskrit adalah bahwa tidak ada gagasan alami tentang kesamaan untuk vektor satu-panas. Dengan demikian, alternatifnya adalah belajar menyandikan kesamaan dalam vektor itu sendiri. Ide intinya adalah bahwa makna kata diberikan oleh kata-kata yang sering muncul di dekatnya.</p><p><strong>Text Embeddings</strong> adalah representasi vektor string bernilai nyata. Kami membangun vektor padat untuk setiap kata, dipilih sehingga mirip dengan vektor kata yang muncul dalam konteks yang sama. Penyematan kata dianggap sebagai titik awal yang bagus untuk sebagian besar tugas NLP yang mendalam. Mereka memungkinkan pembelajaran mendalam menjadi efektif pada kumpulan data yang lebih kecil, karena mereka sering menjadi input pertama untuk arsitektur pembelajaran mendalam dan cara paling populer untuk mentransfer pembelajaran di NLP. Nama yang paling populer dalam penyematan kata adalah <strong>Word2vec</strong> oleh Google (Mikolov) dan <strong>GloVe</strong> oleh Stanford (Pennington, Socher, dan Manning). Mari kita pelajari lebih dalam representasi kata ini</p><p>Di <strong>Word2vec,</strong> kita memiliki korpus teks yang besar di mana setiap kata dalam kosakata tetap diwakili oleh vektor. Kami kemudian pergi melalui setiap posisi <em>t</em> dalam teks, yang memiliki kata tengah <em>c </em>dan kata konteks <em>o</em>. Selanjutnya, kita menggunakan kesamaan vektor kata untuk c dan o untuk menghitung <em>probabilitas o yang diberikan c (atau sebaliknya ). Kami terus menyesuaikan vektor kata untuk memaksimalkan probabilitas ini.</em></p><p>Untuk pelatihan Word2vec yang efisien, kita dapat menghilangkan kata-kata yang tidak berarti (atau frekuensi yang lebih tinggi) dari himpunan data (seperti a, <strong>the</strong>, <strong>of,</strong> <strong>then</strong> ...). Ini membantu meningkatkan akurasi model dan waktu pelatihan. Selain itu, kita dapat menggunakan pengambilan sampel negatif untuk setiap input dengan memperbarui bobot untuk semua label yang benar, tetapi hanya pada sejumlah kecil label yang salah.</p><p>Word2vec memiliki 2 varian model yang layak disebut:</p><p><strong>Lewati-Gram:</strong> Kami mempertimbangkan jendela konteks yang berisi <em>k</em> istilah berturut-turut. Kemudian kita melewatkan salah satu dari kata-kata ini dan mencoba mempelajari jaringan saraf yang mendapatkan semua istilah kecuali yang dilewati dan memprediksi istilah yang dilewati. Oleh karena itu, jika 2 kata berulang kali berbagi konteks yang sama dalam korpus besar, vektor penyematan dari istilah-istilah tersebut akan memiliki vektor yang dekat.</p><p>CBOW dan SkipGram</p><p><strong>Kantong kata-kata terus menerus:</strong> Kami mengambil banyak dan banyak kalimat dalam korpus besar. Setiap kali kita melihat sebuah kata, kita mengambil kata di sekitarnya. Kemudian kita memasukkan kata-kata konteks ke jaringan saraf dan memprediksi kata di tengah konteks ini. Ketika kita memiliki ribuan kata konteks dan kata pusat, kita memiliki satu contoh dataset untuk jaringan saraf. Kami melatih jaringan saraf dan akhirnya, output lapisan tersembunyi yang dikodekan mewakili penyematan untuk kata tertentu. Kebetulan ketika kita melatih ini dalam sejumlah besar kalimat, kata-kata dalam konteks yang sama mendapatkan vektor yang serupa.</p><p>Model <strong>GloVe </strong>berusaha memecahkan masalah ini dengan menangkap makna satu kata yang disematkan dengan struktur seluruh korpus yang diamati. Untuk melakukannya, keluhan model Satu dengan Skip-Gram dan CBOW adalah bahwa keduanya adalah model berbasis jendela, yang berarti statistik kejadian bersama korpus tidak digunakan secara efisien, menghasilkan penanaman yang kurang optimal.</p><p>Model GloVe berusaha memecahkan masalah ini dengan menangkap makna satu kata yang disematkan dengan struktur seluruh korpus yang diamati. Untuk melakukannya, model ini melatih jumlah kata yang terjadi bersamaan secara global dan memanfaatkan statistik secara memadai dengan meminimalkan kesalahan kuadrat terkecil dan, sebagai hasilnya, menghasilkan ruang vektor kata dengan substruktur yang bermakna. Garis besar seperti itu cukup mempertahankan kesamaan kata-kata dengan jarak vektor.</p><p>Selain 2 penyematan teks ini, ada banyak model yang lebih canggih yang dikembangkan baru-baru ini, termasuk FastText, Poincare Embeddings, sense2vec, Skip-Thought, Adaptive Skip-Gram.</p>"
  },
  {
      "title": "Pembelajaran Mendalam NLP",
      "category": [
          "Advanced NLP"
      ],
      "text": "<p>Sebagian besar teknologi NLP ini didukung oleh Deep Learning — subbidang pembelajaran mesin. Deep Learning baru mulai mendapatkan momentum lagi pada awal dekade ini, terutama karena keadaan ini:</p><ul><li><p>Jumlah data pelatihan yang lebih besar.</p></li><li><p>Mesin yang lebih cepat dan CPU/GPU multicore.</p></li><li><p>Model dan algoritma baru dengan kemampuan canggih dan peningkatan kinerja: Pembelajaran representasi menengah yang lebih fleksibel, pembelajaran sistem bersama ujung ke ujung yang lebih efektif, metode pembelajaran yang lebih efektif untuk menggunakan konteks dan mentransfer antar tugas, serta metode regularisasi dan pengoptimalan yang lebih baik.</p></li></ul><p>Sebagian besar metode pembelajaran mesin bekerja dengan baik karena representasi yang dirancang manusia dan fitur input serta pengoptimalan bobot untuk membuat prediksi akhir dengan sebaik-baiknya. Di sisi lain, dalam deep learning, pembelajaran representasi mencoba untuk secara otomatis mempelajari fitur atau representasi yang baik dari input mentah. Fitur yang dirancang secara manual dalam pembelajaran mesin sering kali terlalu ditentukan, tidak lengkap, dan membutuhkan waktu lama untuk merancang dan memvalidasi. Sebaliknya, fitur pembelajaran mendalam mudah beradaptasi dan cepat dipelajari.</p><p>Deep Learning menyediakan kerangka kerja yang sangat fleksibel, universal, dan dapat dipelajari untuk mewakili dunia untuk informasi visual dan linguistik. Awalnya, ini menghasilkan terobosan di bidang-bidang seperti pengenalan suara dan visi komputer. Baru-baru ini, pendekatan pembelajaran mendalam telah memperoleh kinerja yang sangat tinggi di banyak tugas NLP yang berbeda. Model-model ini sering dapat dilatih dengan model end-to-end tunggal dan tidak memerlukan rekayasa fitur tugas khusus tradisional.</p>"
  },
  {
      "title": "Pemrosesan Bahasa Alami: Teknik Lanjutan ~ Analisis Mendalam.",
      "category": [
          "Advanced NLP"
      ],
      "text": "<h1><strong>Apa itu NLP?</strong></h1><p><strong>Natural Language Processing</strong> (NLP)<strong> </strong>adalah bidang di persimpangan ilmu komputer, kecerdasan buatan, dan linguistik. Tujuannya adalah agar komputer memproses atau \"memahami\" bahasa alami untuk melakukan tugas-tugas seperti Terjemahan Bahasa dan Menjawab Pertanyaan.</p><p>Dengan munculnya antarmuka suara dan chatbots, NLP adalah salah satu teknologi terpenting dari era informasi, bagian penting dari kecerdasan buatan. Memahami sepenuhnya dan mewakili makna bahasa adalah tujuan yang sangat sulit. Mengapa? Karena bahasa manusia cukup istimewa.</p><p>Bidang kecerdasan buatan selalu membayangkan mesin mampu meniru fungsi dan kemampuan pikiran manusia. Bahasa dianggap sebagai salah satu pencapaian paling signifikan manusia yang telah mempercepat kemajuan umat manusia. Jadi, tidak mengherankan bahwa ada banyak pekerjaan yang dilakukan untuk mengintegrasikan bahasa ke dalam bidang kecerdasan buatan dalam bentuk Natural Language Processing (NLP). Hari ini kita melihat pekerjaan yang dimanifestasikan dalam orang-orang seperti Alexa dan Siri.</p><p>NLP terutama terdiri daripemahaman Bahasa atural N (manusia ke mesin) dan generasi bahasa alami (mesin ke manusia). Artikel ini terutama akan membahas pemahaman bahasa alami (NLU). Dalam beberapa tahun terakhir telah terjadi lonjakan data tidak terstruktur dalam bentuk teks, video, audio, dan foto. NLU membantu dalam mengekstraksi informasi berharga dari teks seperti data media sosial, survei pelanggan, dan keluhan.</p><p>Apa yang spesial dari bahasa manusia? Beberapa hal sebenarnya:</p><ul><li><p>Bahasa manusia adalah sistem yang secara khusus dibangun untuk menyampaikan makna pembicara / penulis. Ini bukan hanya sinyal lingkungan tetapi komunikasi yang disengaja. Selain itu, ia menggunakan pengkodean yang dapat dipelajari anak-anak kecil dengan cepat; Itu juga berubah.</p></li><li><p>Bahasa manusia sebagian besar merupakan sistem pensinyalan diskrit / simbolis / kategoris, mungkin karena keandalan pensinyalan yang lebih besar.</p></li><li><p>Simbol-simbol kategoris dari suatu bahasa dapat dikodekan sebagai sinyal untuk komunikasi dalam beberapa cara: suara, gerak tubuh, tulisan, gambar, dll bahasa manusia mampu menjadi salah satu dari mereka.</p></li><li><p>Bahasa manusia bersifat ambigu (tidak seperti pemrograman dan bahasa formal lainnya); Dengan demikian ada tingkat kompleksitas yang tinggi dalam mewakili, belajar, dan menggunakan pengetahuan linguistik / situasional / kontekstual / kata / visual terhadap bahasa manusia.</p></li></ul><h1><strong>Mengapa belajar NLP?</strong></h1><p>Ada koleksi aplikasi berguna yang berkembang pesat yang berasal dari bidang studi ini. Mulai dari yang sederhana hingga yang kompleks. Di bawah ini adalah beberapa di antaranya:</p><ul><li><p>Pemeriksaan ejaan, pencarian kata kunci, menemukan sinonim.</p></li><li><p>Mengekstrak informasi dari situs web seperti harga produk, tanggal, lokasi, orang, atau nama perusahaan.</p></li><li><p>Klasifikasi: tingkat membaca teks sekolah, sentimen positif / negatif dari dokumen yang lebih panjang.</p></li><li><p>Terjemahan Mesin.</p></li><li><p>Sistem Dialog Lisan.</p></li><li><p>Jawaban Pertanyaan Kompleks.</p></li></ul><p>Memang, aplikasi ini telah banyak digunakan di industri: dari <strong>pencarian</strong> (tertulis dan lisan) hingga <strong>pencocokan</strong> iklan online; dari <strong>terjemahan</strong> otomatis/bantuan ke <strong>analisis sentimen</strong> untuk pemasaran atau keuangan/perdagangan; dan dari <strong>pengenalan ucapan</strong> hingga <strong>chatbots/agen dialog </strong>(mengotomatiskan dukungan pelanggan, mengontrol perangkat, memesan barang).</p>"
  },
  {
      "title": "Contoh Penerapan NLP",
      "category": [
          "Natural Language Processing (NLP)",
          "Advanced NLP"
      ],
      "text": "<p>Ada banyak contoh penggunaan NLP dalam kehidupan sehari-hari, bahkan bisa jadi kamu telah menggunakannya hampir setiap hari. Di bawah ini contoh penerapan NLP:</p><h3><strong>Pendeteksi spam</strong></h3><p>Pendeteksi spam seperti yang ada pada Google Mail menggunakan teknologi NLP untuk mendeteksi spam melalui klasifikasi kata.</p><h3><strong>Mesin penerjemah</strong></h3><p>Siapa yang belum pernah menggunakan Google Translate? Mesin penerjemah seperti Google Translate, Microsoft Translator, Linguee, dsb akan memudahkan kamu memahami tulisan asing dengan lebih cepat.</p><p><strong>Baca Juga: Apa Itu Bot? Pengertian, Fungsi, dan Contohnya</strong></p><h3><strong>Virtual assistant dan chatbot</strong></h3><p>Asisten virtual seperti Siri dan Alexa menggunakan teknologi speech recognition untuk mengenali setiap suara dan merespons penggunanya dengan tindakan yang sesuai.</p><p>Ada juga <em>chatbot</em> yang dapat mengenali petunjuk kontekstual tentang permintaan manusia. Kemudian menggunakannya untuk memberikan respons atau opsi yang lebih baik dari waktu ke waktu.</p><h3><strong>Analisis sentimen media sosial</strong></h3><p>Analisis sentimen yang dilakukan oleh NLP dapat membantu kamu memahami lebih dalam bahasa yang digunakan pelanggan dalam posting media sosial, tanggapan, ulasan, serta lainnya. Dengan begitu, kamu dapat lebih mudah mengarahkan proses <em>design</em>, kampanye dan lainnya.</p><h3><strong>Text summarization</strong></h3><p>Mesin peringkas teks menggunakan teknologi NLP untuk menganalisis teks digital dalam jumlah besar dan membuat ringkasan dari teks tersebut dengan lebih cepat.</p>"
  },
  {
      "title": "Penerapan NLP pada Chat GPT",
      "category": [
          "Natural Language Processing (NLP)",
          "Advanced NLP"
      ],
      "text": "<p>Chat GPT bekerja dengan memanfaatkan teknologi NLP oleh OpenAI. Menariknya, fitur ini juga Verihubs gunakan dalam produk <strong>Whatsapp Business Messaging</strong> yang mampu membantu Anda dalam menjawab setiap pertanyaan pelanggan dengan cepat dan akurat. Berikut cara kerjanya:</p><h3><strong>1. Pemahaman Token</strong></h3><p>Ketika Anda memberikan masukan teks, Chat GPT memecahnya menjadi “token” kecil, seperti kata, frasa, atau tanda baca. Hal itu merupakan langkah awal pada pemrosesan bahasa.</p><h3><strong>2. Embedding</strong></h3><p>Setiap token melalui proses <em>convert</em> menjadi vektor numerik dalam ruang dimensi atau “<em>embedding</em>“. Hal ini membuat sistem paham akan makna kata-kata berdasarkan konteks dalam kalimat.</p><h3><strong>3. Self-Attention</strong></h3><p>Chat GPT menggunakan mekanisme <em>self-attention</em> dalam arsitektur <em>transformer</em>. Hal tersebut membantu sistem memahami hubungan antara kata-kata dalam sebuah kalimat, serta mengenali pola kompleks dan jarak yang lebih luas.</p><h3><strong>4. Arsitektur Transformer</strong></h3><p>Model <em>transformer</em> terdiri dari beberapa “<em>layer</em>” yang mengandung modul <em>self-attention</em> dan <em>layer</em> terhubung yang lain. Selanjutnya, setiap <em>layer</em> meneruskan informasi dari <em>layer</em> sebelumnya, lalu memprosesnya lebih lanjut, sehingga membantu model memahami makna dan konteks dengan lebih baik.</p><h3><strong>5. Pemrosesan Hierarkis&nbsp;</strong></h3><p>Selanjutnya, <em>transformer</em> memproses teks secara hierarkis. Hal tersebut terjadi dengan setiap lapisan fokus terhadap informasi yang menjadi lebih abstrak, mulai dari kata-kata yang bersifat individu hingga makna dari keseluruhan kalimat.</p><h3><strong>6. Decoding</strong></h3><p>Setelah berhasil melewati lapisan transformer. Kemudian, model menghasilkan representasi internal teks untuk menghasilkan respons dalam bentuk teks.</p><h3><strong>7. Sampling atau Beam Search</strong></h3><p>Ketika menghasilkan respons, model dapat menggunakan teknik sampel probabilitas atau biasanya disebut dengan <em>beam search</em> untuk memilih kata-kata berikutnya. Hasilnya ia mampu menciptakan variasi dalam respons atau respons yang lebih gramatikal.</p><h3><strong>8. Pengembangan Konteks</strong></h3><p>Salah satu keunggulan utama dari Chat GPT adalah kemampuannya untuk merespons dengan lebih baik melalui pengembangan konteks. Dengan demikian, pengguna dapat merasa lebih diperhatikan dan mendapatkan respons yang lebih relevan.</p><h3><strong>9. Hasil yang Didapatkan</strong></h3><p>Akhir dari interaksi dengan Chat GPT adalah ketika pengguna menerima teks respons dari model, sebagai hasil akhirnya. Dengan berbagai tahapan pemrosesan dan pemahaman konteks yang canggih, model ini memberikan respons yang akurat dan relevan kepada pengguna.</p>"
  },
  {
      "title": "Langkah-langkah Utama Data Preprocessing",
      "category": [
          "Pre-processing"
      ],
      "text": "<p>Data preprocessing merupakan teknik yang diterapkan pada database untuk menghapus noise, missing value, error, data yang tidak penting dan data yang tidak konsisten. Tujuan dari data preprocessing yaitu untuk mentranformasikan data mentah untuk dianalisis agar dapat menghasilkan data yang berkualitas dan akurat. Biasanya dalam data realtime database seringkali tidak lengkap dan tidak konsisten sehingga hasil data mining tidak berkualitas dan kurang akurat. Oleh karena itu, untuk meningkatkan kualitas data yang akan dianalisis perlu dilakukan langkah-langkah preprocessing data. Langkah-langkah tersebut tidak harus semuanya dilakukan.</p><p>Berikut langkah-langkah utama dari data preprocessing :</p><ol><li><p>Data Cleaning</p></li></ol><p>Data Cleaning berfungsi untuk mengganti missing value, menormalkan data yang bermasalah (Noisy), mengindentifikasi dan menghilangkan data yang tidak konsisten dan data yang berulang (Redundancy) yang didapat dari integrasi data, dan menyelesaikan masalah inconsistensi data. Oleh karena itu perlu adanya proses pembersihan data atau biasa dikenal dengan data cleaning.</p><ol start=\"2\"><li><p>Data Integration</p></li></ol><p>Data Integration berfungsi untuk mengabungkan beberapa database dan file menjadi 1 sehingga menghasilkan sumber data yang besar.</p><ol start=\"3\"><li><p>Data Transformation</p></li></ol><p>Data Tranformation Berfungsi untuk menormalisasikan data dan aggresi data. Data transformation biasanya digunakan untuk mengubah data dalam bentuk yang sesuai dalam proses data mining.</p><ol start=\"4\"><li><p>Data Reduction</p></li></ol><p>Data Reduction berfungsi untuk mengurangi volume data yang berlebihan tetapi tetap mempertahankan kualitas dari hasil analisis data. Oleh karena itu, perlu adanya teknik data reduction dengan tujuan untuk meningkatkan efisiensi penyimpanan serta mengurangi biaya penyimpanan dan analisis data.</p><ol start=\"5\"><li><p>Data Discretization</p></li></ol><p>Data Discretization berfungsi sebagai bagian dari data reduction dengan memperhitungkan data yang signifikan (Data Numeric).</p>"
  },
  {
      "title": "FUNGSI PREPROCESSING PADA DATA MINING",
      "category": [
          "Pre-processing"
      ],
      "text": "<p><em>Preprocessing </em>data penting untuk dilakukan karena dapat memberikan fungsi atau manfaat pada <em>data mining</em>. Proses ini utamanya dilakukan untuk memastikan kualitas data baik sebelum digunakan saat analisis data. Dalam proses ini Anda dapat memastikan enam hal, yakni akurasi data, kelengkapan, konsistensi, ketepatan waktu, tepercaya, dan dapat diinterpretasi dengan baik.</p><p>Jika sebuah data telah diproses berdasarkan enam acuan tersebut, proses analisis data akan lebih mudah dilakukan karena data dari berbagai sumber telah dimuat dalam sebuah set data dengan format yang sama.</p>"
  },
  {
      "title": "STEP-STEP DALAM DATA PREPROCESSING",
      "category": [
          "Pre-processing"
      ],
      "text": "<p>Setelah mengetahui tentang apa itu data <em>preprocessing </em>, ada beberapa step yang perlu dilakukan ketika akan melakukan <em>data preprocessing</em>. Berikut ini beberapa tahapannya<em>:</em></p><h3><strong>1. <em>DATA CLEANING</em></strong></h3><p>Tahap pertama yang perlu dilakukan ketika akan <em>preprocessing </em>data adalah <em>data cleaning </em>atau membersihkan data. Artinya, data mentah yang telah Anda peroleh perlu diseleksi kembali. Kemudian, hapus atau hilangkan data-data yang tidak lengkap, tidak relevan, dan tidak akurat. Dengan melakukan tahap ini, Anda akan menghindari kesalahpahaman ketika menganalisis data tersebut.</p><p>Ada dua hal yang harus Anda perhatikan ketika melakukan <em>data cleaning</em>, yakni pastikan data-data yang dikumpulkan tidak lagi mengandung data dengan <em>missing values</em>. Lalu, Anda juga harus memastikan bahwa data-data tersebut seluruhnya diperlukan saat proses analisis data. Dengan demikian, data yang Anda kumpulkan telah disesuaikan dan tidak mubazir.</p><h3><strong>2. <em>DATA INTEGRATION</em></strong></h3><p>Karena <em>data preprocessing </em>akan menggabungkan beberapa data dalam suatu <em>dataset</em>, maka Anda harus mengecek data-data yang datang dari berbagai sumber tersebut supaya memiliki format yang sama. Proses ini menjadi salah satu step penting dalam proses ini.</p><p>Beberapa permasalahan bisa muncul ketika melakukan <em>data integration</em>. Misalnya, Anda ingin menggabungkan data dari beberapa sumber. Anda harus mengetahui bahwa data pada sumber pertama dimiliki oleh si A, dan data pada sumber kedua juga terkait dengan si A. Kelihatannya seperti hal mudah, padahal dua sumber tersebut memiliki format yang berbeda. Itulah yang membuat <em>data integration </em>sedikit lebih rumit.</p><h3><strong><em>3. TRANSFORMASI DATA</em></strong></h3><p>Proses berikutnya yang harus dilakukan adalah transformasi data. Seperti yang telah dijelaskan di atas, data akan diambil dari berbagai sumber yang kemungkinan memiliki perbedaan format. Anda harus menyamakan seluruh data yang terkumpul supaya dapat mempermudah proses analisis data.</p><p>Misalnya, Anda akan mengambil data karyawan pada sumber pertama yang menggunakan format DD/MM/YYYY. Kemudian, pada sumber berikutnya, data karyawan menggunakan format MM/DD/YYYY. Ketika akan mengumpulkan data, keduanya tentu perlu diubah dan diseragamkan dalam satu format yang sama.</p><h3><strong><em>4. MENGURANGI DATA</em></strong></h3><p>Tahap terakhir yang perlu dilakukan&nbsp;adalah mengurangi jumlah data (<em>data reduction</em>). Maksudnya adalah Anda harus mengurangi sampel data yang diambil, tetapi dengan catatan, tidak akan mengubah hasil analisis data.</p><p>Ada tiga teknik yang bisa diterapkan saat melakukan pengurangan data, yakni <em>dimensionality reduction </em>(pengurangan dimensi), <em>numerosity reduction </em>(pengurangan jumlah), dan <em>data compression </em>(kompresi data). Ketiga teknik tersebut bisa disesuaikan dengan kebutuhan; apakah data yang diolah besar, sedang, atau perlu dikompresi dan berisiko merugikan.</p>"
  },
  {
      "title": "APA ITU PREPROCESSING DATA?",
      "category": [
          "Pre-processing"
      ],
      "text": "<p><em>Data preprocessing</em> adalah proses yang mengubah data mentah ke dalam bentuk yang lebih mudah dipahami. Proses ini penting dilakukan karena data mentah sering kali tidak memiliki format yang teratur. Selain itu, <em>data mining </em>juga tidak dapat memproses data mentah, sehingga proses ini&nbsp;sangat penting dilakukan untuk mempermudah proses berikutnya, yakni analisis data.</p>"
  },
  {
      "title": "Tahapan Kerja Data Preprocessing",
      "category": [
          "Pre-processing"
      ],
      "text": "<p>Agar dapat berjalan secara optimal, proses kerja <em>data processing</em> dibagi menjadi empat tahap yang berbeda, yakni <em>data cleaning</em>, <em>data integration</em>, <em>data transformation</em>, dan <em>data reduction</em>.</p><h3><strong>1. Data Cleaning</strong></h3><p>Dalam langkah <em>data cleaning</em>, data mentah akan dibersihkan melalui beberapa proses seperti mengisi nilai yang hilang, menghaluskan <em>noisy</em> data, dan menyelesaikan inkonsestensi yang ditemukan.</p><p>Data juga bisa dibersihkan dan dirapikan menggunakan segmen-segmen yang memiliki ukuran serupa lalu dihaluskan (<em>binning</em>), dengan fungsi regresi linear atau berganda (<em>regression</em>), atau dengan mengelompokkannya ke dalam kelompok data yang serupa (<em>grouping</em>).</p><h3><strong>2. Data Integration</strong></h3><p><em>Data integration</em> adalah tahap yang menggabungkan data dari berbagai sumber menjadi satu kesatuan data (<em>dataset</em>). Dalam proses penggabungan tersebut, data dengan format yang berbeda harus diubah terlebih dahulu ke format yang sama. Secara keseluruhan, proses integrasi data ini ditujukan untuk menyatukan dan membuat data menjadi lebih halus melalui upaya-upaya sebagai berikut.</p><ul><li><p>Memastikan data memiliki format dan atribut yang sama</p></li><li><p>Menghapus atribut yang tidak dibutuhkan dari semua sumber data</p></li><li><p>Mendeteksi nilai data yang konflik</p></li></ul><h3><strong>3. Data Transformation</strong></h3><p>Pada tahap ini, data akan dinormalisasi dan digeneralisasi. Normalisasi data dilakukan untuk memastikan bahwa tidak ada data yang berlebihan, sementara generalisasi data dilakukan untuk menyeragamkan data.</p><p>Data transformation memungkinkan Anda untuk mengubah struktur data, format data, dan nilai data menjadi sebuah <em>dataset</em> yang sesuai untuk proses <em>mining</em> ataupun algoritma yang sudah dirancang.</p><p>Terdapat setidaknya lima langkah yang dapat dilakukan dalam proses data transformation, yaitu:</p><ul><li><p><strong>Agregation</strong>: Langkah untuk menggabungkan semua data dalam format yang seragam.</p></li><li><p><strong>Normalization</strong>: Langkah untuk mengubah data ke dalam skala yang teratur sehingga dapat dibandingkan dengan lebih akurat.</p></li><li><p><strong>Feature</strong> <strong>Selection</strong>: Langkah untuk menentukan variabel apa saja yang paling penting untuk analisis, di mana variabel ini juga akan digunakan untuk melatih model <em>machine learning</em> atau kecerdasan buatan.</p></li><li><p><strong>Discreditization</strong>: Langkah untuk mengumpulkan data ke dalam interval yang lebih kecil. Misalnya, saat menghitung latihan harian rata-rata, Anda bisa mengelompokkannya menjadi 0-15 menit, 15-30 menit, dan seterusnya, daripada menggunakan menit dan detik secara rinci.</p></li><li><p><strong>Concept Hierarchy Generation</strong>: Langkah untuk menambahkan hirarki baru di dalam <em>dataset</em>.</p></li></ul><h3><strong>4. Data Reduction</strong></h3><p>Langkah terakhir yang perlu dilakukan adalah <em>data reduction</em> atau pengurangan jumlah data. <em>Data mining</em> menggunakan data dalam jumlah besar yang dikhawatirkan dapat menyebabkan tingkat akurasinya menjadi rendah. Oleh karena itu, sampel data perlu direduksi, namun dengan tetap memperhatikan bahwa proses tersebut tidak akan mengubah hasil analisis data.</p><p>Ada tiga teknik yang bisa diterapkan saat mereduksi data, yakni dengan <em>dimensionality reduction</em> (pengurangan dimensi), <em>numerosity reduction</em> (pengurangan jumlah), dan <em>data</em> <em>compression</em> (kompresi data). Ketiga teknik tersebut bisa disesuaikan dengan kebutuhan, seperti apakah data yang diolah besar, sedang, atau perlu dikompresi dan beresiko merugikan.</p>"
  },
  {
      "title": "Manfaat Data Preprocessing",
      "category": [
          "Pre-processing"
      ],
      "text": "<p>Berdasarkan pengertian di atas, dapat dipahami bahwa <em>data preprocessing </em>berperan penting dalam proyek yang berbasis pada <em>database</em>. Dapat dikatakan pula bahwa <em>data preprocessing</em> memberi sejumlah manfaat bagi proyek ataupun perusahaan seperti:</p><ol><li><p>Memperlancar proses data mining</p></li><li><p>Membuat data lebih mudah untuk dibaca</p></li><li><p>Mengurangi beban representasi dalam data</p></li><li><p>Mengurangi durasi data mining secara signifikan</p></li><li><p>Mempermudah proses analisis data dalam machine learning</p></li></ol>"
  },
  {
      "title": "Apa Itu Data Preprocessing?",
      "category": [
          "Pre-processing"
      ],
      "text": "<p><em>Data preprocessing</em> adalah proses mengubah data mentah ke dalam bentuk yang lebih mudah dipahami. Proses ini diperlukan untuk memperbaiki kesalahan pada data mentah yang seringkali tidak lengkap dan memiliki format yang tidak teratur.</p><p><em>Preprocessing</em> melibatkan proses validasi dan imputasi data. Validasi bertujuan untuk menilai tingkat kelengkapan dan akurasi data yang tersaring. Sedangkan imputasi bertujuan memperbaiki kesalahan dan memasukkan nilai yang hilang, baik secara manual atau otomatis melalui program <em>business process automation</em> (BPA).</p><p>Kualitas data memang berdampak langsung terhadap keberhasilan setiap proyek yang melibatkan analisis data. Dalam <em>machine learning, data preprocessing</em> berperan memastikan bahwa <em>big data</em> sudah diformat dan informasi didalamnya dapat dipahami oleh algoritma perusahaan sehingga bisa mengeluarkan hasil yang lebih akurat.</p>"
  },
  {
      "title": "Teknik dasar classification dalam pre-processing",
      "category": [
          "Pre-processing"
      ],
      "text": "<p><strong>Decision tree model<br></strong>Decision tree adalah algoritma machine learning yang menggunakan seperangkat aturan untuk membuat keputusan dengan struktur seperti pohon yang memodelkan kemungkinan hasil, biaya sumber daya, utilitas dan kemungkinan konsekuensi atau resiko. Konsepnya adalah dengan cara menyajikan algoritma dengan pernyataan bersyarat, yang meliputi cabang untuk mewakili langkah-langkah pengambilan keputusan yang dapat mengarah pada hasil yang menguntungkan.</p><p>Tree merupakan struktur data yang biasanya tidak kontigu, dimana sebuah node bisa memiliki beberapa “anak” (child node), dan berbeda dengan Graph, jalan menuju sebuah&nbsp;child node&nbsp;hanya bisa dicapai melalui maksimal 1 node, dimana pada Graph, dimungkinkan bahwa 1 node bisa dicapai dari banyak node lainnya. Sebuah&nbsp;node&nbsp;yang tidak memiliki&nbsp;child node&nbsp;sama sekali dinamakan&nbsp;leaf node.</p><p><strong>Support Vector Machine<br></strong>Algoritma Support Vector Machine merupakan salah satu algoritma yang termasuk dalam kategori Supervised Learning, yang artinya data yang digunakan untuk belajar oleh mesin merupakan data yang telah memiliki label sebelumnya. Sehingga dalam proses penentuan keputusan, mesin akan mengkategorikan data testing ke dalam label yang sesuai dengan karakteristik yang dimiliki nya.</p><p>Cara kerja dari metode Support Vector Machine khususnya pada masalah non-linear adalah dengan memasukkan konsep kernel ke dalam ruang berdimensi tinggi. Tujuannya adalah untuk mencari hyperplane atau pemisah yang dapat memaksimalkan jarak (margin) antar kelas data. Untuk menemukan hyperplane terbaik, kita dapat mengukur margin kemudian mencari titik maksimalnya. Proses pencarian hyperplane yang terbaik ini adalah ini dari metode Support Vector Machine ini.</p><p><strong>Naive Bayes Classifier<br></strong>Naive Bayes adalah algoritma machine learning untuk masalah klasifikasi. Ini didasarkan pada teorema probabilitas Bayes. Hal ini digunakan untuk klasifikasi teks yang melibatkan set data pelatihan dimensi tinggi. Beberapa contohnya adalah penyaringan spam, analisis sentimental, dan klasifikasi artikel berita.</p><p>Tidak hanya dikenal karena kesederhanaannya, tetapi juga karena keefektifannya. Sangat cepat untuk membangun model dan membuat prediksi dengan algoritma Naive Bayes. Naive Bayes adalah algoritma pertama yang harus dipertimbangkan untuk memecahkan masalah klasifikasi teks.</p><p><strong>Random forest<br></strong><em>Random forest</em>&nbsp;(RF) adalah suatu&nbsp;algoritma&nbsp;yang digunakan pada klasifikasi data dalam jumlah yang besar. Klasifikasi&nbsp;<em>random forest</em>&nbsp;dilakukan melalui penggabungan pohon (<em>tree</em>) dengan melakukan&nbsp;<em>training</em>&nbsp;pada sampel data yang dimiliki. Penggunaan pohon (tree) yang semakin banyak akan mempengaruhi akurasi yang akan didapatkan menjadi lebih baik. Penentuan klasifikasi dengan&nbsp;<em>random forest</em>&nbsp;diambil berdasarkan hasil&nbsp;<em>voting</em>&nbsp;dari&nbsp;<em>tree</em>&nbsp;yang terbentuk. Pemenang dari&nbsp;<em>tree</em>&nbsp;yang terbentuk ditentukan dengan&nbsp;<em>vote</em>&nbsp;terbanyak.</p><p><strong><em>k-Nearest Neighbor</em></strong><br>Algoritma&nbsp;<strong><em>k-Nearest Neighbor</em></strong>&nbsp;adalah algoritma&nbsp;<em>supervised</em> <em>learning</em>&nbsp;dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori&nbsp;<strong>k</strong>-tetangga terdekat. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan&nbsp;<em>sample-sample</em>&nbsp;dari&nbsp;<em>training data</em>. Algoritma&nbsp;<em>k-Nearest Neighbor</em>&nbsp;menggunakan&nbsp;<em>Neighborhood Classification</em>&nbsp;sebagai nilai prediksi dari nilai&nbsp;<em>instance&nbsp;</em>yang baru.</p><p><strong>Logistic Regrresion<br></strong><em>Logistic regression</em>&nbsp;adalah jenis analisis statistik yang sering digunakan&nbsp;<em>data analyst</em>&nbsp;untuk pemodelan prediktif. Dalam pendekatan analitik ini, variabel dependennya terbatas atau kategoris, bisa berupa A atau B (regresi biner) atau berbagai opsi hingga A, B, C atau D (regresi multinomial). Jenis analisis statistik&nbsp;digunakan dalam&nbsp;<em>software</em>&nbsp;statistik untuk memahami hubungan antara variabel dependen dan satu atau lebih variabel independen dengan memperkirakan probabilitas. Jenis analisis ini dapat membantu Anda memprediksi kemungkinan.</p>"
  },
  {
      "title": "Tahapan dalam pre-processing",
      "category": [
          "Natural Language Processing (NLP)",
          "Pre-processing"
      ],
      "text": "<ul><li><p><strong>DATA CLEANING: </strong>Tahap pertama yang perlu dilakukan ketika akan preprocessing data adalah data cleaning atau membersihkan data. Artinya, data mentah yang telah diperoleh perlu diseleksi kembali. Kemudian, hapus atau hilangkan data-data yang tidak lengkap, tidak relevan, dan tidak akurat. Dengan melakukan tahap ini, Anda akan menghindari kesalahpahaman ketika menganalisis data tersebut</p></li><li><p><strong>DATA INTEGRATION: </strong>Karena data preprocessing akan menggabungkan beberapa data dalam suatu dataset, maka kita harus mengecek data-data yang datang dari berbagai sumber tersebut supaya memiliki format yang sama.</p></li><li><p><strong>TRANSFORMASI DATA: </strong>Proses berikutnya yang harus dilakukan adalah transformasi data. Seperti yang telah dijelaskan di atas, data akan diambil dari berbagai sumber yang kemungkinan memiliki perbedaan format. Kita harus menyamakan seluruh data yang terkumpul supaya dapat mempermudah proses analisis data</p></li><li><p><strong>MENGURANGI DATA, </strong>Tahap terakhir yang perlu dilakukan adalah mengurangi jumlah data (data reduction). Maksudnya adalah kita harus mengurangi sampel data yang diambil, tetapi dengan catatan, tidak akan mengubah hasil analisis data. Ada tiga teknik yang bisa diterapkan saat melakukan pengurangan data, yakni dimensionality reduction (pengurangan dimensi), numerosity reduction (pengurangan jumlah), dan data compression (kompresi data).</p></li></ul>"
  },
  {
      "title": "Teknik pre-processing dan classification dalam data science",
      "category": [
          "Natural Language Processing (NLP)",
          "Pre-processing"
      ],
      "text": "<p>Data Preprocessing merupakan salah satu tahapan dalam melakukan mining data. Sebelum menuju ke tahap&nbsp; pemprosesan. Data mentah akan diolah terlebih dahulu. Data Preprocessing atau praproses data biasanya dilakukan melalui cara eliminasi data yang tidak sesuai. Selain itu dalam proses ini data akan diubah dalam bentuk yang akan lebih dipahami oleh sistem.</p><p>Pengertian lain menyebutkan bahwa data preprocessing adalah tahapan untuk menghilangkan beberapa permasalahan yang bisa mengganggu saat pemrosesan data. Hal tersebut karena banyak data yang formatnya tidak konsisten. Data preprocessing merupakan teknik paling awal sebelum melakukan data mining. Namun terdapat beberapa proses juga dalam data preprocessing seperti membersihkan, mengintegrasikan, mentransformasikan dan mereduksi data.</p><p>Melalui data preprocessing, memungkinkan proses mining akan berjalan dengan lebih efektif dan efisien. Karena data yang telah melalui Pra-pemrosesan data, merupakan data yang sudah melalui beberapa tahap pembersihan.</p>"
  },
  {
      "title": "Apa yang dimaksud dengan NLP?",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Pemrosesan bahasa alami (NLP) adalah sebuah teknologi <em>machine learning</em> yang memberi komputer kemampuan untuk menginterpretasikan, memanipulasi, dan memahami bahasa manusia. Banyak organisasi dewasa ini memiliki begitu banyak data suara dan teks dari berbagai saluran komunikasi seperti email, pesan teks, umpan berita media sosial, video, audio, dan banyak lagi. Mereka menggunakan perangkat lunak NLP untuk memproses data ini secara otomatis, menganalisis maksud atau sentimen dalam pesan, dan merespons komunikasi manusia dalam waktu nyata.</p>"
  },
  {
      "title": "Metode Pengolahan Bahasa Alami",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Pada awal perkembangannya, banyak sistem pengolah bahasa didesain dengan metode simbolik, yaitu penyusunan aturan secara manual dengan kamus, misal penyusunan tata bahasa atau aturan heuristik untuk pemotongan kata.[1][2]</p><p>Sejak \"revolusi statistik\"[3][4] pada akhir 1980-an dan pertengahan 1990-an, banyak penelitian pengolahan bahasa alami bergantung pada pemelajaran mesin. Paradigma pemelajaran mesin ini memakai statistika inferensi untuk mempelajari tata bahasa secara otomatis dari sebuah korpus.</p>"
  },
  {
      "title": "Sejarah Pengolahan Bahasa Alami",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Pengolahan bahasa alami berawal pada tahun 1950-an. Pada 1950, Alan Turing memublikasikan artikel yang berjudul \"Computing Machinery and Intelligence\" yang mengusulkan ujian yang sekarang dikenal sebagai uji Turing menjadi salah satu syarat kecerdasan.</p>"
  },
  {
      "title": "Pengolahan Bahasa Alami",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p><strong>Pengolahan bahasa alami</strong> (disingkat <strong>PBA</strong>; bahasa Inggris: <em>natural language processing</em>, disingkat <strong>NLP</strong>) adalah cabang ilmu komputer, linguistik, dan kecerdasan buatan yang mengkaji interaksi antara komputer dan bahasa (alami) manusia, khususnya cara memprogram komputer untuk mengolah data bahasa alami dalam jumlah besar. Hasilnya adalah komputer mampu \"memahami\" isi dokumen, termasuk nuansa bahasa di dalamnya. Dengan ini, komputer dapat dengan akurat mengambil informasi dan wawasan dari dokumen sekaligus mengelompokkan dan menata dokumen-dokumen itu sendiri.</p><p>Kajian NLP antara lain mencakup segmentasi wicara, segmentasi teks, penandaan kelas kata, dan pengawataksaan makna. Meski kajiannya dapat mencakup teks dan wicara, pengolahan wicara telah berkembang menjadi suatu bidang kajian terpisah.</p>"
  },
  {
      "title": "Kembangkan Natural Language Processing untuk Tingkatkan Bisnis Anda Sekarang",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p><em>Peran natural language processing</em> (NLP) di dunia digital ini sangat penting, terutama untuk bisnis memahami kebutuhan pelanggan mereka. Dengan demikian, mereka bisa memberikan apa yang pelanggan butuhkan dengan tepat.</p><p>Misalnya asisten virtual chatbot yang bisa memahami bahasa pelanggan yang terkadang menggunakan bahasa sehari-hari. NLP pada Chatbot memahami bahasa tersebut tanpa perlu pelatihan terlebih dahulu.</p><p>Mekari Qontak menyediakan Chatbot yang terintegrasi NLP. Hal ini tentu akan membantu bisnis untuk membangun hubungan dan meningkatkan pengalaman pelanggan. Selain itu, masih banyak fitur lainnya dari Chatbot AI dari Mekari Qontak untuk mendukung aktivitas bisnis.</p><p>Mekari Qontak telah dipercaya lebih dari <strong>3500 perusahaan </strong>ternama di Indonesia. Tidak hanya itu saja, Mekari Qontak juga telah <strong>tersertifikasi ISO 27001 </strong>untuk memastikan keamanan informasi data pelanggan.</p><p><strong>Jadi tunggu apa lagi? Coba Gratis gratis chatbot AI terintegrasi natural language processing selama 14 hari dari Mekari Qontak, atau konsultasikan kebutuhan bisnis Anda bersama tim support Mekari Qontak!</strong></p>"
  },
  {
      "title": "Contoh Implementasi Natural Language Processing",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Seperti yang telah dijelaskan sebelumnya, natural language processing memiliki berbagai manfaat yang sangat relevan dengan kebutuhan saat ini. Oleh karena itu, banyak bisnis yang tidak melewatkan peluang untuk mengadopsi teknologi NLP dalam aktivitas sehari-hari mereka.</p><p>Berikut adalah contoh-contoh produk yang menggunakan teknologi <em>natural language processing</em> sebagai komponen pendukung:</p><h3><strong>1. Asisten Virtual Chatbot</strong></h3><p>Beberapa orang mungkin sudah familiar dengan chatbot. Chatbot adalah teknologi yang dapat memberikan respon otomatis dalam waktu singkat terhadap pesan yang diterima.</p><p>Dengan adopsi teknologi ini pada chatbot, respon yang diberikan menjadi lebih personal dan bukan sekadar template bot. Teknologi ini juga memungkinkan chatbot untuk memahami pesan dengan bahasa yang umum digunakan oleh manusia.</p><p>Kemampuan tersebut secara signifikan meningkatkan pengalaman pelanggan (<em>customer experience)</em>.</p><p><code>Baca juga: 15 Aplikasi Chatbot Terbaik untuk Bisnis Digital 2023</code></p><h3><strong>2. Media Sosial</strong></h3><p>Popularitas media sosial (medsos) seperti Instagram, TikTok, Facebook, dan lainnya. Yang menarik, platform medsos ini juga menggunakan teknologi <em>Natural Language Processing</em> (NLP).</p><p>Dalam konteks bisnis, penerapan NLP pada media sosial membantu dalam menganalisis bahasa yang digunakan dalam postingan, termasuk tanggapan, ulasan, dan lain-lain.</p><p>Insight yang dihasilkan dari analisis tersebut sangat berharga untuk memahami perilaku dan emosi pengguna terhadap produk, promosi, atau acara tertentu. Dengan informasi tersebut, memudahkan bisnis dalam membuat desain produk, kampanye iklan, dan lain sebagainya.</p><h3><strong>3. Aplikasi Ojek Online</strong></h3><p>Saat ini, aplikasi ojek online seperti Gojek, Grab, Uber, dan lainnya telah menjadi bagian penting dari kehidupan sehari-hari. Aplikasi ini memberikan kenyamanan bagi pengguna dalam berpindah dari satu lokasi ke lokasi lainnya.</p><p>Ternyata kemudahan tersebut tak lepas dari dukungan teknologi NLP pada aplikasi ojek online. NLP berperan dalam menemukan dan mengidentifikasi nama lokasi dengan bahasa yang lebih akrab dan mudah dipahami oleh pengguna.</p><p>Selain menunjukkan lokasi, NLP juga memberikan informasi lebih mendetail tentang tempat-tempat tertentu seperti area parkir mobil, lokasi lobby, dan lain-lain.</p><p>Dengan adanya teknologi NLP ini, pengguna aplikasi ojek online dapat dengan mudah menentukan tujuan dan mendapatkan informasi yang dibutuhkan untuk perjalanan mereka.</p><h3><strong>4. Manajemen Rumah Sakit</strong></h3><p>Rumah sakit menjadi salah satu lokasi dengan tingkat kesibukan tinggi dan banyak dokumen yang perlu dikelola. Misalnya, identitas pasien dan catatan riwayat kesehatan yang memerlukan waktu lama bagi tim medis untuk mencari data secara manual setiap kali ada pasien yang datang untuk pemeriksaan.</p><p>Oleh karena itu, banyak rumah sakit saat ini mengadopsi aplikasi berbasis NLP. Tujuan dari penggunaan teknologi ini adalah untuk menyimpan semua dokumen penting dalam sistem terpadu sehingga memudahkan dan mempercepat proses pencarian saat informasi tersebut dibutuhkan.</p><p>Dengan menggunakan teknologi NLP, tim medis dapat dengan mudah mengakses dan menemukan dokumen-dokumen tersebut tanpa harus melalui proses pencarian manual yang memakan waktu.</p>"
  },
  {
      "title": "Apa Manfaat Natural Language Processing?",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Dikutip dari laman Google cloud bahwa NLP berfungsi untuk mengolah data berbasis teks yang tidak terstruktur dan memberikan wawasan yang lebih dalam. Dengan menggunakan NLP, Anda dapat mengakses informasi yang diekstraksi dari data tersebut dan mendapatkan pemahaman baru yang berarti dari data yang ada.</p><p>Manfaat lain dari natural language processing adalah:</p><ul><li><p>Meningkatkan akurasi analisis data untuk mengurangi risiko kesalahan dalam menafsirkan data.</p></li><li><p>Mampu meneliti ratusan hingga ribuan data secara bersamaan dalam waktu singkat.</p></li><li><p>Mengurangi biaya analisis dengan sistem otomatis yang dapat beroperasi 24 jam non-stop.</p></li><li><p>Lebih baik dalam memahami sentimen pasar melalui analisis ratusan review pelanggan secara online.</p></li><li><p>Mampu menyortir komplain pelanggan berdasarkan topik atau tingkat urgensi untuk memberikan solusi secara cepat, meningkatkan kepuasan pelanggan.</p></li></ul>"
  },
  {
      "title": "Bagaimana Cara Kerja Natural Language Processing?",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Natural language processing (NLP) bekerja dengan sistem semi otomatis. Pada tahap awal penggunaan, Anda perlu mengajarkan mesin atau teknologi NLP untuk memahami data dengan benar.</p><p>Kemudian, mesin akan mengembangkan algoritma khusus untuk secara mandiri memahami kata-kata yang ditemui. Setelah algoritma terbentuk, NLP dapat dikategorikan menjadi beberapa jenis berikut:</p><ul><li><p><em>Part-of-speech tagging</em>: Memproses kata-kata sesuai dengan kelompok katanya, seperti kata kerja, kata sifat, atau kata benda.</p></li><li><p><em>Stop word removal</em>: Memilih kata-kata unik yang memiliki informasi penting untuk dianalisis dengan menghapus kata-kata umum.</p></li><li><p><em>Lemmatization and stemming</em>: Menghapus imbuhan baik di awal maupun akhir kata untuk mendapatkan kata dasar.</p></li><li><p><em>Tokenization</em>: Memecah kata-kata menjadi unit-unit lebih kecil sebelum diproses.</p></li></ul><p>Setelah algoritma terbentuk, mesin akan melaksanakan tugas sesuai dengan perintah yang diberikan, misalnya merespons perintah dalam bentuk teks atau suara..</p>"
  },
  {
      "title": "Apa saja Jenis Natural Language Processing?",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p>Ada dua jenis sistem natural language yang umum digunakan sebagai berikut:</p><h3><strong>1. Rules-based system</strong></h3><p>Sistem rules-based merupakan algoritma yang telah ada sejak awal pengembangan NLP dan masih digunakan hingga saat ini. Jenis NLP ini mengandalkan aturan linguistik yang dirancang secara cermat.</p><h3><strong>2. Machine learning-based system</strong></h3><p>Sistem<em> machine learning </em>memanfaatkan metode statistik untuk model pemrosesan NLP. Teknologi ini secara otomatis melakukan tugas sesuai dengan data pelatihan yang diberikan sebelumnya. Algoritma ini akan disesuaikan saat ada penambahan data yang lebih banyak yang diproses.</p>"
  },
  {
      "title": "Apa itu Natural Language Processing?",
      "category": [
          "Natural Language Processing (NLP)",
          "Introduction"
      ],
      "text": "<p><em>Natural Language Processing</em> (NLP) adalah suatu teknologi kecerdasan buatan yang memiliki kemampuan untuk menginterpretasikan, memanipulasi, dan memahami bahasa manusia.</p><p>Teknologi NLP menggabungkan bidang linguistik komputasi, pembelajaran mesin, dan model pembelajaran mendalam untuk memproses bahasa manusia.</p><p>Tidak hanya terbatas pada teks atau suara, NLP juga mampu memproses data yang diinputkan oleh manusia melalui berbagai saluran komunikasi, seperti email, pesan teks, media sosial, video, audio, dan lainnya.</p><p>Setiap kali manusia mengirimkan data melalui saluran-saluran tersebut, NLP secara otomatis menganalisis maksud atau sentimen dalam pesan tersebut, dan merespons komunikasi manusia dalam waktu nyata.</p><p>Karena itulah, NLP memainkan peran penting dalam dunia bisnis dengan membantu merampingkan operasional bisnis, meningkatkan produktivitas karyawan, dan menyederhanakan proses bisnis.</p>"
  }
]